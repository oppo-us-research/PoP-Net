"""Data Preparation for generic 3d pose align network on ITOP dataset"""import copyimport loggingimport osimport numpy as npimport jsonimport matplotlib.pyplot as pltimport cv2import randomfrom random import uniformfrom glob import globimport torch.utils.dataimport torchvisionfrom PIL import Imagefrom lib.datasets.heatmap import putGaussianMapsfrom lib.datasets.paf import putVecMapsfrom lib.datasets.posemap import putJointZ, putJointDxDyfrom lib.datasets import data_augmentation_2d3dfrom lib.utils.common import draw_humans_visibility, pos_3d_from_2d_and_depthfrom lib.utils.prior_pose_align import parse_prior_poseplt.rcParams['figure.figsize'] = (9, 6)# joint_id_to_name = {#   0: 'Head',        8: 'Torso',#   1: 'Neck',        9: 'R Hip',#   2: 'R Shoulder',  10: 'L Hip',#   3: 'L Shoulder',  11: 'R Knee',#   4: 'R Elbow',     12: 'L Knee',#   5: 'L Elbow',     13: 'R Foot',#   6: 'R Hand',      14: 'L Foot',#   7: 'L Hand',# }# TODO: setup manually, load inside dataset laterintrinsics = {'fx': 504.1189880371094, 'fy': 504.042724609375, 'cx': 231.7421875, 'cy': 320.62640380859375}jointColors = [[255, 0, 0], [255, 85, 0], [255, 170, 0], [255, 255, 0], [170, 255, 0], [85, 255, 0], [0, 255, 0],               [0, 255, 85], [0, 255, 170], [0, 255, 255], [0, 170, 255], [0, 85, 255], [0, 0, 255], [85, 0, 255],               [170, 0, 255], [255, 0, 255], [255, 0, 170], [255, 0, 85], [85, 255, 85]]root_joint = 'torso'depth_mean = 3depth_std = 2depth_max = 6joint2box_margin = 25# manually desin a few reasonable configurations of locationaug_mods = [[0, 3], [1, 2], [0, 1], [2, 3], [4]]def kp_connections(keypoints):    kp_lines = [        [keypoints.index('torso'), keypoints.index('right_hip')],        [keypoints.index('right_hip'), keypoints.index('right_knee')],        [keypoints.index('right_knee'), keypoints.index('right_ankle')],        [keypoints.index('torso'), keypoints.index('left_hip')],        [keypoints.index('left_hip'), keypoints.index('left_knee')],        [keypoints.index('left_knee'), keypoints.index('left_ankle')],        [keypoints.index('torso'), keypoints.index('neck')],        [keypoints.index('neck'), keypoints.index('right_shoulder')],        [keypoints.index('right_shoulder'), keypoints.index('right_elbow')],        [keypoints.index('right_elbow'), keypoints.index('right_wrist')],        [keypoints.index('neck'), keypoints.index('left_shoulder')],        [keypoints.index('left_shoulder'), keypoints.index('left_elbow')],        [keypoints.index('left_elbow'), keypoints.index('left_wrist')],        [keypoints.index('neck'), keypoints.index('head')]    ]    return kp_linesdef get_joint2chn():    """    compute the corresponding chn in posedepth map for each joint    """    joint_names = get_keypoints()    limb_ids = kp_connections(joint_names)    root_id = joint_names.index(root_joint)    joint2chn = np.zeros(len(joint_names)).astype(np.int)    joint2chn[root_id] = 0    for k, limb in enumerate(limb_ids):        joint2chn[limb[1]] = k+1    return joint2chndef get_keypoints():    """Get the itop keypoints"""    keypoints = [        'head',        'neck',        'right_shoulder',        'left_shoulder',        'right_elbow',        'left_elbow',        'right_wrist',        'left_wrist',        'torso',        'right_hip',        'left_hip',        'right_knee',        'left_knee',        'right_ankle',        'left_ankle']    return keypointsdef get_swap_part_indices():    keypoints = get_keypoints()    swap_indices = []    for keypoint in keypoints:        if keypoint == 'right_shoulder':            swap_indices.append(keypoints.index('left_shoulder'))            continue        if keypoint == 'left_shoulder':            swap_indices.append(keypoints.index('right_shoulder'))            continue        if keypoint == 'right_elbow':            swap_indices.append(keypoints.index('left_elbow'))            continue        if keypoint == 'left_elbow':            swap_indices.append(keypoints.index('right_elbow'))            continue        if keypoint == 'right_wrist':            swap_indices.append(keypoints.index('left_wrist'))            continue        if keypoint == 'left_wrist':            swap_indices.append(keypoints.index('right_wrist'))            continue        if keypoint == 'right_hip':            swap_indices.append(keypoints.index('left_hip'))            continue        if keypoint == 'left_hip':            swap_indices.append(keypoints.index('right_hip'))            continue        if keypoint == 'right_knee':            swap_indices.append(keypoints.index('left_knee'))            continue        if keypoint == 'left_knee':            swap_indices.append(keypoints.index('right_knee'))            continue        if keypoint == 'right_ankle':            swap_indices.append(keypoints.index('left_ankle'))            continue        if keypoint == 'left_ankle':            swap_indices.append(keypoints.index('right_ankle'))            continue        swap_indices.append(keypoints.index(keypoint))    return swap_indicesclass KDH3D_Keypoints(torch.utils.data.Dataset):    """Kinect Depth Human 3D (KDH3D) Dataset.    Caches preprocessing.    Args:        img_dir (string): Root directory where images are saved.        ann_file (string): Path to json annotation file.        transform (callable, optional): A function/transform that  takes in an PIL image            and returns a transformed version. E.g, ``transforms.ToTensor``        target_transform (callable, optional): A function/transform that takes in the            target and transforms it.    """    def __init__(self, img_dir, ann_file, is_train=True, preprocess=None, w_org=480, h_org=512, input_x=224, input_y=224,                 stride=8, stride_z=None, stride_a=None, stride_prior=None, anchors=None,                 pose_align=False, z_radius=2, align_radius=2, bg_aug=False, bg_file=None, bg_dir=None, seg_dir=None):        self.img_dir = img_dir        # load all the ids and joint annotations        self.anno_dic = json.load(open(ann_file, 'r'))        self.ids = [key for key, value in self.anno_dic.items() if key != 'intrinsics']        self.bg_aug = bg_aug        if bg_aug:            self.bg_list = list(json.load(open(bg_file, 'r')).values())            random.shuffle(self.bg_list)            self.bg_dir = bg_dir            self.seg_dir = seg_dir            self.num_bg_images = len(self.bg_list)        self.is_train = is_train        # preprocess for data augmentation, at original image values        self.preprocess = preprocess        # standard totensor and normalize, apply after data augmentation        self.image_transform = torchvision.transforms.Compose(            [torchvision.transforms.ToTensor(),             torchvision.transforms.Normalize(mean=[depth_mean], std=[depth_std])])        self.joint_names = get_keypoints()        self.limb_ids = kp_connections(self.joint_names)        self.root_id = self.joint_names.index(root_joint)        self.joint2chn = get_joint2chn()        self.num_joints = len(self.joint_names)        self.w_org = w_org        self.h_org = h_org        self.input_y = input_y        self.input_x = input_x                self.stride = stride        if stride_z is None:            self.strideZ = stride        else:            self.strideZ = stride_z        if stride_a is None:            self.strideA = stride        else:            self.strideA = stride_a        if stride_prior is None:            self.stride_prior = stride * 2        else:            self.stride_prior = stride_prior        if anchors is None:            self.anchors = np.array([(6., 3.), (12., 6.)])        else:            self.anchors = np.array(anchors)        self.pose_align = pose_align        self.z_radius = z_radius        self.align_radius = align_radius        self.log = logging.getLogger(self.__class__.__name__)    def __getitem__(self, index):        """        Args:            index (int): Index        Returns:            tuple: Tuple (image, target). target is the object returned by ``itop.loadAnns``.        """        """           part 1: standard data augmentation        """        image_id = self.ids[index]        anns = self.anno_dic[image_id]        anns = copy.deepcopy(anns)        image = np.load(os.path.join(self.img_dir, image_id)).astype(np.float)        if self.bg_aug:            bg_id = index % self.num_bg_images            bg_image_path = os.path.join(self.bg_dir, self.bg_list[bg_id]["file_name"])            bg_image = np.load(bg_image_path)            fg_mask = np.load(os.path.join(self.seg_dir, image_id)).astype(np.float)            # compose fg bg depth map using fg mask            image = image * fg_mask + bg_image * (np.ones_like(fg_mask) - fg_mask)        """        part 2: standard data augmentation        """        image, anns = self.preprocess((image, anns))        # data normalization        image[image < 0] = 0        image[image > depth_max] = depth_max        depth_resize = cv2.resize(image,                                  (np.int(self.input_x/self.strideZ), np.int(self.input_y/self.strideZ)))        image = self.image_transform(image)        if not self.is_train:            return image, index        """        part 3: data preparation for the ground-truth of specific dense map representations        """        heatmaps, pafs, zmaps, fg_masks_z, align_maps, fg_masks_align, prior_map, prior_mask_conf, prior_mask_coord, prior_weight_map = \            self.single_image_processing(anns, depth_resize)        return image, heatmaps, pafs, zmaps, fg_masks_z, align_maps, fg_masks_align, prior_map, prior_mask_conf, prior_mask_coord, prior_weight_map, index    def single_image_processing(self, anns, depth_resize):        """        :param anns: 2d and 3d annotations        :param depth_resize: it is a copy of image, but in numpy format for convenience        :return:        """        # prepare target maps        heatmaps, pafs, zmaps, fg_masks_z, align_maps, fg_masks_align,\        prior_map, prior_mask_conf, prior_mask_coord, prior_weight_map = \            self.get_ground_truth(anns, depth_resize)                heatmaps = torch.from_numpy(heatmaps.transpose((2, 0, 1)).astype(np.float32))        pafs = torch.from_numpy(pafs.transpose((2, 0, 1)).astype(np.float32))        zmaps = torch.from_numpy(zmaps.transpose((2, 0, 1)).astype(np.float32))        fg_masks_z = torch.from_numpy(fg_masks_z.transpose((2, 0, 1)).astype(np.float32))        align_maps = torch.from_numpy(align_maps.transpose((2, 0, 1)).astype(np.float32))        fg_masks_align = torch.from_numpy(fg_masks_align.transpose((2, 0, 1)).astype(np.float32))        prior_map = torch.from_numpy(prior_map.transpose((2, 0, 1)).astype(np.float32))        prior_mask_conf = torch.from_numpy(prior_mask_conf.transpose((2, 0, 1)).astype(np.float32))        prior_mask_coord = torch.from_numpy(prior_mask_coord.transpose((2, 0, 1)).astype(np.float32))        prior_weight_map = torch.from_numpy(prior_weight_map.transpose((2, 0, 1)).astype(np.float32))        return heatmaps, pafs, zmaps, fg_masks_z, align_maps, fg_masks_align, prior_map, prior_mask_conf, prior_mask_coord, prior_weight_map    def remove_illegal_joint(self, keypoints_2d, visibility=None):        if visibility is None:            visibility = np.ones([keypoints_2d.shape[0], keypoints_2d.shape[1]])        mask = np.logical_or.reduce((keypoints_2d[:, :, 0] >= self.input_x,                                     keypoints_2d[:, :,  0] < 0,                                     keypoints_2d[:, :, 1] >= self.input_y,                                     keypoints_2d[:, :, 1] < 0))        visibility[mask] = 0        return visibility    def build_prior_targets(self, ground_truth, weights, height, width, noobject_scale=0.1,                 object_scale=1.0):        """        This is the function to prepare anchor based pose representation for the prior subnetwork        ATTENTION: confidence mask needs to consider bg, but coordinate regression should only focus on fg.        Args:            ground_truth:            weights:            height: res of the prior map            width: res of the prior map        Returns:        """        num_anchors = self.anchors.shape[0]        # gt prior map that consistent with network output        gt_prior_map = np.zeros((height, width, num_anchors, (5 + self.num_joints * 3)))        # conf inference considers both bg and fg        fg_mask_conf = np.ones((height, width, num_anchors)) * noobject_scale        # coord inference only focuses on foreground        fg_mask_coord = np.zeros((height, width, num_anchors))        # consider bg as 1 (not its role). differentiable fg pose weights        gt_weight_map = np.ones((height, width, num_anchors))        anchors = np.concatenate([np.zeros_like(self.anchors), self.anchors], 1)        gt_box = np.zeros((len(ground_truth), 4))        # Need to convert ground-truth bbox into [xmin, ymin, h, w] format        for i, anno in enumerate(ground_truth):            gt_box[i, 0] = (anno[0] + anno[2]) / 2 / self.stride_prior            gt_box[i, 1] = (anno[1] + anno[3]) / 2 / self.stride_prior            gt_box[i, 2] = (anno[2] - anno[0]) / self.stride_prior            gt_box[i, 3] = (anno[3] - anno[1]) / self.stride_prior            anno[5:5 + 2 * self.num_joints] /= self.stride_prior        # Find best anchor for each ground truth        # Only rely on the box size to determine the assignment        gt_wh = np.copy(gt_box)        gt_wh[:, :2] = 0        iou_gt_anchors = self.bbox_ious(gt_wh, anchors)        best_anchors = np.argmax(iou_gt_anchors, axis=1)        # Set masks and target values for each ground truth        for i, anno in enumerate(ground_truth):            best_n = best_anchors[i]            gi = min(width - 1, max(0, int(gt_box[i, 0])))            gj = min(height - 1, max(0, int(gt_box[i, 1])))            fg_mask_conf[gj, gi, best_n] = object_scale            fg_mask_coord[gj, gi, best_n] = 1            gt_weight_map[gj, gi, :] = weights[i]            gt_prior_map[gj, gi, best_n, 0] = gt_box[i, 0] - gi            gt_prior_map[gj, gi, best_n, 1] = gt_box[i, 1] - gj            gt_prior_map[gj, gi, best_n, 2] = gt_box[i, 2] / self.anchors[best_n, 0]            gt_prior_map[gj, gi, best_n, 3] = gt_box[i, 3] / self.anchors[best_n, 1]            gt_prior_map[gj, gi, best_n, 4] = 1  # gt anchor-based iou may not be accurate, so just use 0/1            # compute x,y coord canonical to current center            gt_prior_map[gj, gi, best_n, 5:] = anno[5:]            gt_prior_map[gj, gi, best_n, 5:5 + self.num_joints] -= gi            gt_prior_map[gj, gi, best_n, 5 + self.num_joints:5 + 2 * self.num_joints] -= gj            # normalize differently with different anchor sizes.            gt_prior_map[gj, gi, best_n, 5:5 + self.num_joints] /= (self.anchors[best_n, 0] / 2)            gt_prior_map[gj, gi, best_n, 5 + self.num_joints:5 + 2 * self.num_joints] /= (self.anchors[best_n, 1] / 2)        gt_prior_map = gt_prior_map.reshape((height, width, -1))        return gt_prior_map, fg_mask_conf, fg_mask_coord, gt_weight_map    def bbox_ious(self, boxes1, boxes2):        b1x1, b1y1 = np.hsplit(boxes1[:, :2] - (boxes1[:, 2:4] / 2), 2)        b1x2, b1y2 = np.hsplit(boxes1[:, :2] + (boxes1[:, 2:4] / 2), 2)        b2x1, b2y1 = np.hsplit(boxes2[:, :2] - (boxes2[:, 2:4] / 2), 2)        b2x2, b2y2 = np.hsplit(boxes2[:, :2] + (boxes2[:, 2:4] / 2), 2)        # broadcast        b1x1_cast = np.tile(b1x1.reshape(-1, 1), (1, len(b2x1)))        b1y1_cast = np.tile(b1y1.reshape(-1, 1), (1, len(b2y1)))        b1x2_cast = np.tile(b1x2.reshape(-1, 1), (1, len(b2x2)))        b1y2_cast = np.tile(b1y2.reshape(-1, 1), (1, len(b2y2)))        b2x1_cast = np.tile(b2x1.reshape(1, -1), (len(b1x1), 1))        b2y1_cast = np.tile(b2y1.reshape(1, -1), (len(b1y1), 1))        b2x2_cast = np.tile(b2x2.reshape(1, -1), (len(b1x2), 1))        b2y2_cast = np.tile(b2y2.reshape(1, -1), (len(b1y2), 1))        dx = np.minimum(b1x2_cast, b2x2_cast) - np.maximum(b1x1_cast, b2x1_cast)        dy = np.minimum(b1y2_cast, b2y2_cast) - np.maximum(b1y1_cast, b2y1_cast)        dx[dx < 0] = 0        dy[dy < 0] = 0        intersections = dx * dy        areas1 = (b1x2_cast - b1x1_cast) * (b1y2_cast - b1y1_cast)        areas2 = (b2x2_cast - b2x1_cast) * (b2y2_cast - b2y1_cast)        unions = (areas1 + areas2) - intersections        return intersections / unions    # This is the key function to prepare a set of target maps    def get_ground_truth(self, anns, depth_input):        grid_h = int(self.input_y / self.stride)        grid_w = int(self.input_x / self.stride)        Zgrid_h = int(self.input_y / self.strideZ)        Zgrid_w = int(self.input_x / self.strideZ)        Agrid_h = int(self.input_y / self.strideA)        Agrid_w = int(self.input_x / self.strideA)        prior_h = int(self.input_y / self.stride_prior)        prior_w = int(self.input_x / self.stride_prior)        heatmaps = np.zeros((int(grid_h), int(grid_w), self.num_joints + 1))        pafs = np.zeros((int(grid_h), int(grid_w), 2 * len(self.limb_ids)))        # ATTENTION: initialize with max and only work on foreground. At last update background with the union fg mask        # initialize pose depth with input depth map        zmaps_org = np.repeat(depth_input[:, :, np.newaxis], len(self.joint_names), axis=2)        zmaps = np.ones_like(zmaps_org) * 2*depth_max        fg_masks_z = np.zeros((int(Zgrid_h), int(Zgrid_w), len(self.joint_names)))        align_maps = np.zeros((int(Agrid_h), int(Agrid_w), 2*len(self.joint_names)))        fg_masks_align = np.zeros((int(Agrid_h), int(Agrid_w), 2*len(self.joint_names)))        max_dist = 2 * (self.align_radius + 0.5)        dist_maps = np.ones((int(Agrid_h), int(Agrid_w), len(self.joint_names))).astype(np.float32) * max_dist        # This considers multiple annotations per frame later        keypoints_2d = []        keypoints_3d = []        pose_weights = []        objects = []        for ann in anns:            keypoints_2d.append(np.array(ann['2d_joints']).reshape(-1, 2))            keypoints_3d.append(np.array(ann['3d_joints']).reshape(-1, 3))            if 'pose_weight' in ann.keys():                pose_weights.append(ann['pose_weight'])            else:                pose_weights.append(1.0)            object = np.array([np.min(ann['bbox'][0]), np.min(ann['bbox'][1]),                               np.max(ann['bbox'][2]), np.max(ann['bbox'][3]), 1.0])            object = np.concatenate([object, ann['2d_joints'][:, 0].ravel()])            object = np.concatenate([object, ann['2d_joints'][:, 1].ravel()])            object = np.concatenate([object, ann['3d_joints'][:, 2].ravel()])            object[5 + 2 * self.num_joints: 5 + 3 * self.num_joints] -= depth_mean            object[5 + 2 * self.num_joints: 5 + 3 * self.num_joints] /= depth_std            objects.append(object)        keypoints_2d = np.array(keypoints_2d)        inbounds = self.remove_illegal_joint(keypoints_2d)  # indicate inbound of image        keypoints_3d = np.array(keypoints_3d)        # pose_weights = np.array([pose_weights[0]])        objects = np.array(objects)        # prepare prior map and associated masks and weights        prior_map, prior_mask_conf, prior_mask_coord, prior_weight_map = \            self.build_prior_targets(objects, pose_weights, prior_h, prior_w)        """            Notes:                Truncation and occlusion should be considered separately                Occluded part needs to be prepared in regression maps so that they can be predicted in its channel.                This is ambiguity in multi-person overlapping of the same joint type.                So it require to label the occlusion caused by same joint type or others.        """        # confidence maps for body parts        for i in range(self.num_joints):            joints = keypoints_2d[:, i, :]            for j, joint in enumerate(joints):                # consider visible and within-range joint                if inbounds[j, i] > 0.5:                    root_center = joint[:2]                    gaussian_map = heatmaps[:, :, i]                    heatmaps[:, :, i] = putGaussianMaps(                        root_center, gaussian_map,                        7.0, grid_h, grid_w, self.stride)        # background: the last dimension of the heatmap prepared for background. Default for cross-entropy loss        heatmaps[:, :, -1] = np.maximum(            1 - np.max(heatmaps[:, :, :self.num_joints], axis=2),            0.        )        # pafs for limbs        for i, (k1, k2) in enumerate(self.limb_ids):            # limb            count = np.zeros((int(grid_h), int(grid_w)), dtype=np.uint32)            for j, joints in enumerate(keypoints_2d):                if inbounds[j, k1] > 0.5 and inbounds[j, k2] > 0.5:                    centerA = joints[k1, :2]                    centerB = joints[k2, :2]                    vec_map = pafs[:, :, 2 * i:2 * (i + 1)]                    pafs[:, :, 2 * i:2 * (i + 1)], count = putVecMaps(                        centerA=centerA,                        centerB=centerB,                        accumulate_vec_map=vec_map,                        count=count, grid_y=grid_h, grid_x=grid_w, stride=self.stride                    )        # Z maps        for j, joints in enumerate(keypoints_2d):            for k, joint in enumerate(joints):                if inbounds[j, k] < 0.5:                    continue                joint_center = joint[:2]                joint_depth = keypoints_3d[j, k, 2]                map_z = zmaps[:, :, k]                fg_mask_z = fg_masks_z[:, :, k]                zmaps[:, :, k], fg_masks_z[:, :, k] = putJointZ(                    center=joint_center,                    depth=joint_depth,                    accumulate_map=map_z,                    accumulate_mask=fg_mask_z,                    grid_y=Zgrid_h, grid_x=Zgrid_w, stride=self.strideZ, radius=self.z_radius, max_depth=depth_max                )        # align maps        if self.pose_align:            for j, joints in enumerate(keypoints_2d):                for k, joint in enumerate(joints):                    if inbounds[j, k] < 0.5:                        continue                    joint_center = joint[:2]                    joint_depth = keypoints_3d[j, k, 2]                    fg_mask_align = fg_masks_align[:, :, 2 * k: 2 * (k + 1)]                    mapDxDy = align_maps[:, :, 2 * k: 2 * (k + 1)]                    align_maps[:, :, 2 * k: 2 * (k + 1)],\                    fg_masks_align[:, :, 2 * k: 2 * (k + 1)],\                    dist_maps[:, :, k] = putJointDxDy(                        center=joint_center,                        depth=joint_depth,                        accumulate_map=mapDxDy,                        accumulate_mask=fg_mask_align,                        accumulate_dist=dist_maps[:, :, k],                        z_map=zmaps[:, :, k],                        grid_y=Agrid_h, grid_x=Agrid_w, stride=self.strideA, radius=self.align_radius, max_dist=max_dist                    )        # apply zmap bg        zmaps[fg_masks_z == 0] = zmaps_org[fg_masks_z == 0]        # normalize Z map        zmaps[zmaps < 0] = 0        zmaps[zmaps > depth_max] = depth_max        zmaps -= depth_mean        zmaps /= depth_std        return heatmaps, pafs, zmaps, fg_masks_z, align_maps, fg_masks_align, prior_map, prior_mask_conf, prior_mask_coord, prior_weight_map    def __len__(self):        return len(self.ids)# unit test for dataloaderif __name__ == "__main__":    vis = True    w_org = 480    h_org = 512    input_size = 224    num_joints = 15    num_bones = 14    dim_reduction = 8    dim_reductionZ = 8    dim_reductionA = 8    batch_size = 5    PoseAlign = True    ZRadius = 2    AlignRadius = 2    bg_aug = True    anchors = [(6., 3.), (12., 6.)]    joint2chn = np.array(range(num_joints))    DATA_DIR = '/media/yuliang/DATA/Datasets/Kinect_Depth_Human3D'    img_dir = os.path.join(DATA_DIR, 'depth_maps')    ANNOTATIONS = '{}/labels_test.json'.format(os.path.join(DATA_DIR, 'labels'))    bg_file = os.path.join(DATA_DIR, 'labels', 'labels_bg.json')    bg_dir = os.path.join(DATA_DIR, 'bg_maps')    seg_dir = os.path.join(DATA_DIR, 'seg_maps')    preprocess = data_augmentation_2d3d.Compose([        data_augmentation_2d3d.Cvt2ndarray(),        data_augmentation_2d3d.Rotate(cx=intrinsics['cx'], cy=intrinsics['cy']),        data_augmentation_2d3d.RenderDepth(cx=intrinsics['cx'], cy=intrinsics['cy']),        # data_augmentation_depth_3d.Hflip(swap_indices=get_swap_part_indices()),        data_augmentation_2d3d.Crop(),  # it may violate the 3D-2D geometry        data_augmentation_2d3d.Resize(input_size)    ])    val_data = KDH3D_Keypoints(        img_dir=img_dir,        ann_file=ANNOTATIONS,        preprocess=preprocess,        h_org=h_org,        w_org=w_org,        input_x=input_size,        input_y=input_size,        stride=dim_reduction,        stride_z=dim_reductionZ,        stride_a=dim_reductionA,        z_radius=ZRadius,        anchors=anchors,        pose_align=PoseAlign,        bg_aug=bg_aug,        bg_file=bg_file,        bg_dir=bg_dir,        seg_dir=seg_dir    )    val_loader = torch.utils.data.DataLoader(        val_data, batch_size=batch_size, shuffle=True,        pin_memory=True, num_workers=0, drop_last=True)  # set num_wrkers=1 for debugging    cnt = 0    for i, (img, heatmap_target, paf_target, zmap_target, fg_masks_z, alignmap_target, fg_masks_align, prior_map, prior_mask_conf, prior_mask_coord, prior_weight_map, index) in enumerate(val_loader):        # verify 2D pos + Z branch recovers 3D pose,        zmap_target = zmap_target.cpu().data.numpy().transpose(0, 2, 3, 1)        zmap_target[:, :, :, :num_joints] *= depth_std        zmap_target[:, :, :, :num_joints] += depth_mean        alignmap_target = alignmap_target.cpu().data.numpy().transpose(0, 2, 3, 1)        # prior_map = prior_map.cpu().data.numpy().transpose(0, 2, 3, 1)        prior_map = prior_map.cuda()        # evaluate the and visualize direct prior results        bboxes, humans_prior, visibilities = parse_prior_pose(prior_map,                                                              anchors,                                                              num_joints,                                                              input_size,                                                              input_size,                                                              depth_mean,                                                              depth_std,                                                              conf_threshold=0.1)        if vis is True:            for b in range(batch_size):                single_img = img[b, 0, :, :].numpy()                single_heatmap = heatmap_target[b, ...].numpy()                single_paf = paf_target[b, ...].numpy()                single_zmap = zmap_target[b, ...].transpose(2, 0, 1)                single_fg_mask_z = fg_masks_z[b, ...].numpy()                single_alignmap = alignmap_target[b, ...].transpose(2, 0, 1)                single_fg_mask_align = fg_masks_align[b, ...].numpy()                # normalize depth image for visualization                single_img *= depth_std                single_img += depth_mean                single_img[single_img <= 0] = 0                single_img[single_img >= depth_max] = depth_max                single_img /= depth_max                fig1 = plt.figure()                ax1 = fig1.add_subplot(241)                ax1.imshow(single_img)                ax1.set_title('depth input')                # visualize heatmap                ht_max = np.max(single_heatmap[:-1, :, :], axis=0)                ax2 = fig1.add_subplot(242)                ax2.imshow(ht_max)                ax2.set_title('ht max')                # visualize paf map                paf_x = single_paf[0:2 * num_bones:2, ...]                paf_x = np.sum(paf_x, axis=0)                paf_y = single_paf[1:2 * num_bones:2, ...]                paf_y = np.sum(paf_y, axis=0)                ax3 = fig1.add_subplot(243)                ax3.imshow(paf_x)                ax3.set_title('paf x')                ax4 = fig1.add_subplot(247)                ax4.imshow(paf_y)                ax4.set_title('paf y')                # visualize zmap                # ATTENTION: simply visualizing min may keep the noisy part.                zmap_vis = np.min(single_zmap, axis=0)                zmap_vis[zmap_vis <= 0] = 0                zmap_vis[zmap_vis >= depth_max] = depth_max                zmap_vis /= depth_max                ax5 = fig1.add_subplot(245)                ax5.imshow(zmap_vis)                ax5.set_title('poseD head')                # draw humans on the image                single_img *= 255                single_img = cv2.cvtColor(single_img.astype(np.uint8), cv2.COLOR_GRAY2BGR)                humans_2d = [humans_prior[b][hi][:, :2] for hi in range(len(humans_prior[b]))]                # for j in range(len(bboxes[b])):                #     single_bbox = bboxes[b][j]                #     cv2.rectangle(single_img, (int(single_bbox[0]), int(single_bbox[1])),                #                   (int(single_bbox[2]), int(single_bbox[3])), [0, 255, 0], 2)                single_img = draw_humans_visibility(single_img,                                                    humans_2d,                                                    kp_connections(get_keypoints()),                                                    jointColors)                save_name = '{:08d}.png'.format(cnt)                cv2.imwrite(save_name, single_img)                single_img = np.flip(single_img, 2)                ax6 = fig1.add_subplot(246)                ax6.imshow(single_img)                ax6.set_title('skeleton')                if PoseAlign:                    # visualize dx dy field maps, using sum for simplicity, but not correct for occlusion                    dx_field = single_alignmap[0: 2 * num_joints:2, ...]                    dx_field = np.sum(dx_field, axis=0)                    dy_field = single_alignmap[1: 2 * num_joints:2, ...]                    dy_field = np.sum(dy_field, axis=0)                    ax7 = fig1.add_subplot(244)                    ax7.imshow(dx_field)                    ax7.set_title('dx field')                    ax8 = fig1.add_subplot(248)                    ax8.imshow(dy_field)                    ax8.set_title('dy field')                fig1.subplots_adjust(wspace=0.1, hspace=0.01)                # print('pose weight: {}'.format(pose_weights[b]))                fig1.show()                plt.waitforbuttonpress()                plt.pause(1)                plt.close(fig1)        cnt += 1        print('iter: [{}/{}]\t'.format(i, len(val_loader)))