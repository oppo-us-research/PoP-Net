"""Data Preparation for generic 3d pose align network on ITOP dataset"""import copyimport loggingimport osimport numpy as npimport jsonimport matplotlib.pyplot as pltimport cv2import randomimport torch.utils.dataimport torchvisionfrom PIL import Imagefrom lib.datasets.heatmap import putGaussianMapsfrom lib.datasets.paf import putVecMapsfrom lib.datasets.posemap import putJointZfrom lib.datasets import data_augmentation_2d3dfrom lib.utils.common import draw_humans, pos_3d_from_2d_and_depthplt.rcParams['figure.figsize'] = (6, 3)# joint_id_to_name = {#   0: 'Head',        8: 'Torso',#   1: 'Neck',        9: 'R Hip',#   2: 'R Shoulder',  10: 'L Hip',#   3: 'L Shoulder',  11: 'R Knee',#   4: 'R Elbow',     12: 'L Knee',#   5: 'L Elbow',     13: 'R Foot',#   6: 'R Hand',      14: 'L Foot',#   7: 'L Hand',# }# TODO: temporal setup manually, load inside dataset laterintrinsics = {'fx': 504.1189880371094, 'fy': 504.042724609375, 'cx': 231.7421875, 'cy': 320.62640380859375}jointColors = [[255, 0, 0], [255, 85, 0], [255, 170, 0], [255, 255, 0], [170, 255, 0], [85, 255, 0], [0, 255, 0],               [0, 255, 85], [0, 255, 170], [0, 255, 255], [0, 170, 255], [0, 85, 255], [0, 0, 255], [85, 0, 255],               [170, 0, 255], [255, 0, 255], [255, 0, 170], [255, 0, 85], [85, 255, 85]]root_joint = 'torso'depth_mean = 3depth_std = 2depth_max = 6joint2box_margin = 25def kp_connections(keypoints):    kp_lines = [        [keypoints.index('torso'), keypoints.index('right_hip')],        [keypoints.index('right_hip'), keypoints.index('right_knee')],        [keypoints.index('right_knee'), keypoints.index('right_ankle')],        [keypoints.index('torso'), keypoints.index('left_hip')],        [keypoints.index('left_hip'), keypoints.index('left_knee')],        [keypoints.index('left_knee'), keypoints.index('left_ankle')],        [keypoints.index('torso'), keypoints.index('neck')],        [keypoints.index('neck'), keypoints.index('right_shoulder')],        [keypoints.index('right_shoulder'), keypoints.index('right_elbow')],        [keypoints.index('right_elbow'), keypoints.index('right_wrist')],        [keypoints.index('neck'), keypoints.index('left_shoulder')],        [keypoints.index('left_shoulder'), keypoints.index('left_elbow')],        [keypoints.index('left_elbow'), keypoints.index('left_wrist')],        [keypoints.index('neck'), keypoints.index('head')]    ]    return kp_linesdef get_joint2chn():    """    compute the corresponding chn in posedepth map for each joint    """    joint_names = get_keypoints()    limb_ids = kp_connections(joint_names)    root_id = joint_names.index(root_joint)    joint2chn = np.zeros(len(joint_names)).astype(np.int)    joint2chn[root_id] = 0    for k, limb in enumerate(limb_ids):        joint2chn[limb[1]] = k+1    return joint2chndef get_keypoints():    """Get the itop keypoints"""    keypoints = [        'head',        'neck',        'right_shoulder',        'left_shoulder',        'right_elbow',        'left_elbow',        'right_wrist',        'left_wrist',        'torso',        'right_hip',        'left_hip',        'right_knee',        'left_knee',        'right_ankle',        'left_ankle']    return keypointsdef get_swap_part_indices():    keypoints = get_keypoints()    swap_indices = []    for keypoint in keypoints:        if keypoint == 'right_shoulder':            swap_indices.append(keypoints.index('left_shoulder'))            continue        if keypoint == 'left_shoulder':            swap_indices.append(keypoints.index('right_shoulder'))            continue        if keypoint == 'right_elbow':            swap_indices.append(keypoints.index('left_elbow'))            continue        if keypoint == 'left_elbow':            swap_indices.append(keypoints.index('right_elbow'))            continue        if keypoint == 'right_wrist':            swap_indices.append(keypoints.index('left_wrist'))            continue        if keypoint == 'left_wrist':            swap_indices.append(keypoints.index('right_wrist'))            continue        if keypoint == 'right_hip':            swap_indices.append(keypoints.index('left_hip'))            continue        if keypoint == 'left_hip':            swap_indices.append(keypoints.index('right_hip'))            continue        if keypoint == 'right_knee':            swap_indices.append(keypoints.index('left_knee'))            continue        if keypoint == 'left_knee':            swap_indices.append(keypoints.index('right_knee'))            continue        if keypoint == 'right_ankle':            swap_indices.append(keypoints.index('left_ankle'))            continue        if keypoint == 'left_ankle':            swap_indices.append(keypoints.index('right_ankle'))            continue        swap_indices.append(keypoints.index(keypoint))    return swap_indicesclass KDH3D_Keypoints(torch.utils.data.Dataset):    """Kinect Depth Human 3D (KDH3D) Dataset.    Caches preprocessing.    Args:        img_dir (string): Root directory where images are saved.        ann_file (string): Path to json annotation file.        transform (callable, optional): A function/transform that  takes in an PIL image            and returns a transformed version. E.g, ``transforms.ToTensor``        target_transform (callable, optional): A function/transform that takes in the            target and transforms it.    """    def __init__(self, img_dir, ann_file, preprocess=None, input_x=288,  input_y=288, stride=16, stride_z=None, z_radius=2, bg_aug=False, bg_file=None, bg_dir=None, seg_dir=None):        self.img_dir = img_dir        # load all the ids and joint annotations        self.anno_dic = json.load(open(ann_file, 'r'))        self.ids = [key for key, value in self.anno_dic.items() if key != 'intrinsics']        self.bg_aug = bg_aug        if bg_aug:            self.bg_list = list(json.load(open(bg_file, 'r')).values())            random.shuffle(self.bg_list)            self.bg_dir = bg_dir            self.seg_dir = seg_dir            self.num_bg_images = len(self.bg_list)        print('Images: {}'.format(len(self.ids)))        # preprocess for data augmentation, at original image values        self.preprocess = preprocess        # standard totensor and normalize, apply after data augmentation        self.image_transform = torchvision.transforms.Compose(            [torchvision.transforms.ToTensor(),             torchvision.transforms.Normalize(mean=[depth_mean], std=[depth_std])])        self.joint_names = get_keypoints()        self.limb_ids = kp_connections(self.joint_names)        self.root_id = self.joint_names.index(root_joint)        # self.joint2chn = get_joint2chn()        self.num_joints = len(self.joint_names)        self.input_x = input_x        self.input_y = input_y        self.stride = stride        if stride_z is None:            self.strideZ = stride        else:            self.strideZ = stride_z        self.z_radius = z_radius        self.log = logging.getLogger(self.__class__.__name__)    def __getitem__(self, index):        """        Args:            index (int): Index        Returns:            tuple: Tuple (image, target). target is the object returned by ``itop.loadAnns``.        """        image_id = self.ids[index]        anns = self.anno_dic[image_id]        anns = copy.deepcopy(anns)        image = np.load(os.path.join(self.img_dir, image_id)).astype(np.float)        if self.bg_aug:            bg_id = index % self.num_bg_images            bg_image_path = os.path.join(self.bg_dir, self.bg_list[bg_id]["file_name"])            bg_image = np.load(bg_image_path)            fg_mask = np.load(os.path.join(self.seg_dir, image_id)).astype(np.float)            # compose fg bg depth map using fg mask            image = image * fg_mask + bg_image * (np.ones_like(fg_mask) - fg_mask)        # data augmentation        image, anns = self.preprocess((image, anns))        # data normalization        image[image < 0] = 0        image[image > depth_max] = depth_max        depth_resize = cv2.resize(image,                                  (np.int(self.input_x/self.strideZ), np.int(self.input_y/self.strideZ)))        image = self.image_transform(image)        # ground-truth maps preparation        heatmaps, pafs, zmaps, fg_masks_z = self.single_image_processing(anns, depth_resize)        return image, heatmaps, pafs, zmaps, fg_masks_z, anns[0]['2d_joints'], index    def single_image_processing(self, anns, depth_resize):        """        :param anns: 2d and 3d annotations        :param depth_resize: it is a copy of image, but in numpy format for convenience        :return:        """        # prepare target maps        heatmaps, pafs, zmaps, fg_masks_z = self.get_ground_truth(anns, depth_resize)                heatmaps = torch.from_numpy(heatmaps.transpose((2, 0, 1)).astype(np.float32))        pafs = torch.from_numpy(pafs.transpose((2, 0, 1)).astype(np.float32))        zmaps = torch.from_numpy(zmaps.transpose((2, 0, 1)).astype(np.float32))        fg_masks_z = torch.from_numpy(fg_masks_z.transpose((2, 0, 1)).astype(np.float32))        return heatmaps, pafs, zmaps, fg_masks_z    def remove_illegal_joint(self, keypoints_2d, visibility=None):        if visibility is None:            visibility = np.ones([keypoints_2d.shape[0], keypoints_2d.shape[1]])        mask = np.logical_or.reduce((keypoints_2d[:, :, 0] >= self.input_x,                                     keypoints_2d[:, :,  0] < 0,                                     keypoints_2d[:, :, 1] >= self.input_y,                                     keypoints_2d[:, :, 1] < 0))        visibility[mask] = 0        return visibility    # This is the key function to prepare a set of target maps    def get_ground_truth(self, anns, depth_input):        grid_h = int(self.input_y / self.stride)        grid_w = int(self.input_x / self.stride)        Zgrid_h = int(self.input_y / self.strideZ)        Zgrid_w = int(self.input_x / self.strideZ)        heatmaps = np.zeros((int(grid_h), int(grid_w), self.num_joints + 1))        pafs = np.zeros((int(grid_h), int(grid_w), 2 * len(self.limb_ids)))        # ATTENTION: initialize with max and only work on foreground. At last update background with the union fg mask        # initialize pose depth with input depth map        zmaps_org = np.repeat(depth_input[:, :, np.newaxis], len(self.joint_names), axis=2)        zmaps = np.ones_like(zmaps_org) * 2*depth_max        fg_masks_z = np.zeros((int(Zgrid_h), int(Zgrid_w), len(self.joint_names)))        # This considers multiple annotations per frame later        keypoints_2d = []        keypoints_3d = []        for ann in anns:            keypoints_2d.append(np.array(ann['2d_joints']).reshape(-1, 2))            keypoints_3d.append(np.array(ann['3d_joints']).reshape(-1, 3))        keypoints_2d = np.array(keypoints_2d)        keypoints_3d = np.array(keypoints_3d)        inbounds = self.remove_illegal_joint(keypoints_2d)  # indicate inbound of image        """            Notes:                Truncation and occlusion should be considered separately                Occluded part needs to be prepared in regression maps so that they can be predicted in its channel.                This is ambiguity in multi-person overlapping of the same joint type.                So it require to label the occlusion caused by same joint type or others.        """        # confidence maps for body parts        for i in range(self.num_joints):            joints = keypoints_2d[:, i, :]            for j, joint in enumerate(joints):                # consider visible and within-range joint                if inbounds[j, i] > 0.5:                    root_center = joint[:2]                    gaussian_map = heatmaps[:, :, i]                    heatmaps[:, :, i] = putGaussianMaps(                        root_center, gaussian_map,                        7.0, grid_h, grid_w, self.stride)        # background: the last dimension of the heatmap prepared for background. Default for cross-entropy loss        heatmaps[:, :, -1] = np.maximum(            1 - np.max(heatmaps[:, :, :self.num_joints], axis=2),            0.        )        # pafs for limbs        for i, (k1, k2) in enumerate(self.limb_ids):            # limb            count = np.zeros((int(grid_h), int(grid_w)), dtype=np.uint32)            for j, joints in enumerate(keypoints_2d):                if inbounds[j, k1] > 0.5 and inbounds[j, k2] > 0.5:                    centerA = joints[k1, :2]                    centerB = joints[k2, :2]                    vec_map = pafs[:, :, 2 * i:2 * (i + 1)]                    pafs[:, :, 2 * i:2 * (i + 1)], count = putVecMaps(                        centerA=centerA,                        centerB=centerB,                        accumulate_vec_map=vec_map,                        count=count, grid_y=grid_h, grid_x=grid_w, stride=self.stride                    )        # Z maps and align maps        for j, joints in enumerate(keypoints_2d):            for k, joint in enumerate(joints):                if inbounds[j, k] < 0.5:                    continue                joint_center = joint[:2]                joint_depth = keypoints_3d[j, k, 2]                map_z = zmaps[:, :, k]                fg_mask_z = fg_masks_z[:, :, k]                zmaps[:, :, k], fg_masks_z[:, :, k] = putJointZ(                    center=joint_center,                    depth=joint_depth,                    accumulate_map=map_z,                    accumulate_mask=fg_mask_z,                    grid_y=Zgrid_h, grid_x=Zgrid_w, stride=self.strideZ, radius=self.z_radius, max_depth=depth_max                )        # apply zmap bg        zmaps[fg_masks_z == 0] = zmaps_org[fg_masks_z == 0]        # normalize Z map        zmaps[zmaps < 0] = 0        zmaps[zmaps > depth_max] = depth_max        zmaps -= depth_mean        zmaps /= depth_std        return heatmaps, pafs, zmaps, fg_masks_z    def __len__(self):        return len(self.ids)# unit test for dataloaderif __name__ == "__main__":    vis = True    input_size = 224    num_joints = 15    num_bones = 14    dim_reduction = 8    dim_reductionZ = 8    batch_size = 15    ZRadius = 2    bg_aug = False    # joint2chn = get_joint2chn()    joint2chn = np.array(range(num_joints))    DATA_DIR = '/media/yuliang/DATA/Datasets/Kinect_Depth_Human3D'    img_dir = os.path.join(DATA_DIR, 'depth_maps')    ANNOTATIONS_TRAIN = [os.path.join(DATA_DIR, 'labels', item) for item in ['labels_train.json']]    bg_file = os.path.join(DATA_DIR, 'labels', 'labels_bg.json')    bg_dir = os.path.join(DATA_DIR, 'bg_maps')    seg_dir = os.path.join(DATA_DIR, 'seg_maps')    preprocess = data_augmentation_2d3d.Compose([        data_augmentation_2d3d.Cvt2ndarray(),        data_augmentation_2d3d.Rotate(cx=intrinsics['cx'], cy=intrinsics['cy']),        data_augmentation_2d3d.RenderDepth(cx=intrinsics['cx'], cy=intrinsics['cy']),        # data_augmentation_depth_3d.Hflip(swap_indices=get_swap_part_indices()),        data_augmentation_2d3d.Crop(),  # it may violate the 3D-2D geometry        data_augmentation_2d3d.Resize(input_size)    ])    train_data = KDH3D_Keypoints(        img_dir=img_dir,        ann_file=ANNOTATIONS_TRAIN[0],        preprocess=preprocess,        input_x=input_size,        input_y=input_size,        stride=dim_reduction,        stride_z=dim_reductionZ,        z_radius=ZRadius,        bg_aug=bg_aug,        bg_file=bg_file,        bg_dir=bg_dir,        seg_dir=seg_dir    )    train_loader = torch.utils.data.DataLoader(        train_data, batch_size=batch_size, shuffle=True,        pin_memory=True, num_workers=0, drop_last=True)  # set num_wrkers=1 for debugging    for i, (img, heatmap_target, paf_target, zmap_target, fg_masks_z, anns, index) in enumerate(train_loader):        # verify 2D pos + Z branch recovers 3D pose,        # TODO: verification is currently not compatible with augmentation        zmap_target = zmap_target.cpu().data.numpy().transpose(0, 2, 3, 1)        zmap_target[:, :, :, :num_joints] *= depth_std        zmap_target[:, :, :, :num_joints] += depth_mean        humans_gt_2d_batch = []        humans_gt_3d_batch = []        humans_gt_vis_batch = []        for id in index.numpy():            human_gt_batch = train_data.anno_dic[train_data.ids[id]]            human_gt_2d = [human['2d_joints'] for human in human_gt_batch]            humans_gt_2d_batch.append(human_gt_2d)            human_gt_3d = [human['3d_joints'] for human in human_gt_batch]            humans_gt_3d_batch.append(human_gt_3d)        if vis is True:            for j in range(batch_size):                single_img = img[j, 0, :, :].numpy()                single_heatmap = heatmap_target[j, ...].numpy()                single_paf = paf_target[j, ...].numpy()                single_zmap = zmap_target[j, ...].transpose(2, 0, 1)                single_fg_mask_z = fg_masks_z[j, ...].numpy()                single_anno = [anns[j, ...].numpy()]  # ATTENTION: this anno goes through data augmentation                # normalize depth image for visualization                single_img *= 2                single_img += 3                single_img[single_img <= 0] = 0                single_img[single_img >= 5] = 5                single_img /= 5                fig1 = plt.figure()                ax1 = fig1.add_subplot(241)                ax1.imshow(single_img)                ax1.set_title('depth input')                # visualize heatmap                ht_max = np.max(single_heatmap[:-1, :, :], axis=0)                ax2 = fig1.add_subplot(242)                ax2.imshow(ht_max)                ax2.set_title('ht max')                # visualize paf map                paf_x = single_paf[0:2 * num_bones:2, ...]                paf_x = np.sum(paf_x, axis=0)                paf_y = single_paf[1:2 * num_bones:2, ...]                paf_y = np.sum(paf_y, axis=0)                ax3 = fig1.add_subplot(243)                ax3.imshow(paf_x)                ax3.set_title('paf x')                ax4 = fig1.add_subplot(247)                ax4.imshow(paf_y)                ax4.set_title('paf y')                # visualize zmap                # ATTENTION: simply visualizing min may keep the noisy part.                zmap_vis = np.min(single_zmap, axis=0)                zmap_vis[zmap_vis <= 0] = 0                zmap_vis[zmap_vis >= depth_max] = depth_max                zmap_vis /= depth_max                ax5 = fig1.add_subplot(245)                ax5.imshow(zmap_vis)                ax5.set_title('poseD head')                # draw humans on the image                single_img *= 255                single_img = cv2.cvtColor(single_img.astype(np.uint8), cv2.COLOR_GRAY2BGR)                single_img = draw_humans(single_img,                                         single_anno,                                         kp_connections(get_keypoints()),                                         jointColors)                single_img = np.flip(single_img, 2)                ax6 = fig1.add_subplot(246)                ax6.imshow(single_img)                ax6.set_title('skeleton')                fig1.subplots_adjust(wspace=0.1, hspace=0.01)                fig1.show()                plt.waitforbuttonpress()                plt.pause(1)                plt.close(fig1)        print('iter: [{}/{}]\t'.format(i, len(train_loader)))