"""Data Preparation for generic 3d pose align network on ITOP dataset"""import copyimport loggingimport osimport numpy as npimport jsonimport matplotlib.pyplot as pltimport cv2import randomfrom random import uniformfrom glob import globimport torch.utils.dataimport torchvisionfrom PIL import Imagefrom lib.datasets.heatmap import putGaussianMapsfrom lib.datasets.paf import putVecMapsfrom lib.datasets.posemap import putJointZ, putJointDxDyfrom lib.datasets import data_augmentation_2d3dfrom lib.utils.common import *from lib.utils.prior_pose_align import parse_prior_poseplt.rcParams['figure.figsize'] = (9, 6)# joint_id_to_name = {#   0: 'Head',        8: 'Torso',#   1: 'Neck',        9: 'R Hip',#   2: 'R Shoulder',  10: 'L Hip',#   3: 'L Shoulder',  11: 'R Knee',#   4: 'R Elbow',     12: 'L Knee',#   5: 'L Elbow',     13: 'R Foot',#   6: 'R Hand',      14: 'L Foot',#   7: 'L Hand',# }# TODO: temporal setup manually, load inside dataset laterintrinsics = {'fx': 504.1189880371094, 'fy': 504.042724609375, 'cx': 231.7421875, 'cy': 320.62640380859375}jointColors = [[255, 0, 0], [255, 85, 0], [255, 170, 0], [255, 255, 0], [170, 255, 0], [85, 255, 0], [0, 255, 0],               [0, 255, 85], [0, 255, 170], [0, 255, 255], [0, 170, 255], [0, 85, 255], [0, 0, 255], [85, 0, 255],               [170, 0, 255], [255, 0, 255], [255, 0, 170], [255, 0, 85], [85, 255, 85]]root_joint = 'torso'depth_mean = 3depth_std = 2depth_max = 6joint2box_margin = 25# manually desin a few reasonable configurations of locationaug_mods = [[0, 3], [1, 2], [0, 1], [2, 3], [4]]# aug_mods = [[0], [1], [2], [3], [4]]def kp_connections(keypoints):    kp_lines = [        [keypoints.index('torso'), keypoints.index('right_hip')],        [keypoints.index('right_hip'), keypoints.index('right_knee')],        [keypoints.index('right_knee'), keypoints.index('right_ankle')],        [keypoints.index('torso'), keypoints.index('left_hip')],        [keypoints.index('left_hip'), keypoints.index('left_knee')],        [keypoints.index('left_knee'), keypoints.index('left_ankle')],        [keypoints.index('torso'), keypoints.index('neck')],        [keypoints.index('neck'), keypoints.index('right_shoulder')],        [keypoints.index('right_shoulder'), keypoints.index('right_elbow')],        [keypoints.index('right_elbow'), keypoints.index('right_wrist')],        [keypoints.index('neck'), keypoints.index('left_shoulder')],        [keypoints.index('left_shoulder'), keypoints.index('left_elbow')],        [keypoints.index('left_elbow'), keypoints.index('left_wrist')],        [keypoints.index('neck'), keypoints.index('head')]    ]    return kp_linesdef get_joint2chn():    """    compute the corresponding chn in posedepth map for each joint    """    joint_names = get_keypoints()    limb_ids = kp_connections(joint_names)    root_id = joint_names.index(root_joint)    joint2chn = np.zeros(len(joint_names)).astype(np.int)    joint2chn[root_id] = 0    for k, limb in enumerate(limb_ids):        joint2chn[limb[1]] = k+1    return joint2chndef get_keypoints():    """Get the itop keypoints"""    keypoints = [        'head',        'neck',        'right_shoulder',        'left_shoulder',        'right_elbow',        'left_elbow',        'right_wrist',        'left_wrist',        'torso',        'right_hip',        'left_hip',        'right_knee',        'left_knee',        'right_ankle',        'left_ankle']    return keypointsdef get_swap_part_indices():    keypoints = get_keypoints()    swap_indices = []    for keypoint in keypoints:        if keypoint == 'right_shoulder':            swap_indices.append(keypoints.index('left_shoulder'))            continue        if keypoint == 'left_shoulder':            swap_indices.append(keypoints.index('right_shoulder'))            continue        if keypoint == 'right_elbow':            swap_indices.append(keypoints.index('left_elbow'))            continue        if keypoint == 'left_elbow':            swap_indices.append(keypoints.index('right_elbow'))            continue        if keypoint == 'right_wrist':            swap_indices.append(keypoints.index('left_wrist'))            continue        if keypoint == 'left_wrist':            swap_indices.append(keypoints.index('right_wrist'))            continue        if keypoint == 'right_hip':            swap_indices.append(keypoints.index('left_hip'))            continue        if keypoint == 'left_hip':            swap_indices.append(keypoints.index('right_hip'))            continue        if keypoint == 'right_knee':            swap_indices.append(keypoints.index('left_knee'))            continue        if keypoint == 'left_knee':            swap_indices.append(keypoints.index('right_knee'))            continue        if keypoint == 'right_ankle':            swap_indices.append(keypoints.index('left_ankle'))            continue        if keypoint == 'left_ankle':            swap_indices.append(keypoints.index('right_ankle'))            continue        swap_indices.append(keypoints.index(keypoint))    return swap_indicesclass KDH3D_Keypoints(torch.utils.data.Dataset):    """Kinect Depth Human 3D (KDH3D) Dataset.    Caches preprocessing.    Args:        img_dir (string): Root directory where images are saved.        ann_file (string): Path to json annotation file.        transform (callable, optional): A function/transform that  takes in an PIL image            and returns a transformed version. E.g, ``transforms.ToTensor``        target_transform (callable, optional): A function/transform that takes in the            target and transforms it.    """    def __init__(self, img_dir, ann_file_list, preprocess=None, w_org=480, h_org=512, input_x=224, input_y=224,                 stride=8, stride_z=None, stride_a=None, stride_prior=None, anchors=None,                 z_radius=2, align_radius=2, depth_thresh=0.03,                 bg_file=None, bg_dir=None, seg_dir=None, pred_vis=False):        self.img_dir = img_dir        # load all the ids and joint annotations        self.anno_dic_list = []        self.ids_list = []        for ann_file in ann_file_list:            self.anno_dic_list.append(json.load(open(ann_file, 'r')))            self.ids_list.append([key for key, value in self.anno_dic_list[-1].items() if key != 'intrinsics'])            random.shuffle(self.ids_list[-1])            print('Images: {}'.format(len(self.ids_list[-1])))        self.dataset_len = np.max([len(self.ids_list[ii]) for ii in range(len(self.ids_list))])        self.bg_list = list(json.load(open(bg_file, 'r')).values())        random.shuffle(self.bg_list)        self.bg_dir = bg_dir        self.seg_dir = seg_dir        self.num_bg_images = len(self.bg_list)        # preprocess for data augmentation, at original image values        self.preprocess = preprocess        # standard totensor and normalize, apply after data augmentation        self.image_transform = torchvision.transforms.Compose(            [torchvision.transforms.ToTensor(),             torchvision.transforms.Normalize(mean=[depth_mean], std=[depth_std])])        self.joint_names = get_keypoints()        self.limb_ids = kp_connections(self.joint_names)        self.root_id = self.joint_names.index(root_joint)        self.joint2chn = get_joint2chn()        self.num_joints = len(self.joint_names)        self.w_org = w_org        self.h_org = h_org        self.input_y = input_y        self.input_x = input_x                self.stride = stride        if stride_z is None:            self.strideZ = stride        else:            self.strideZ = stride_z        if stride_a is None:            self.strideA = stride        else:            self.strideA = stride_a        if stride_prior is None:            self.stride_prior = stride * 2        else:            self.stride_prior = stride_prior        if anchors is None:            self.anchors = np.array([(6., 3.), (12., 6.)])        else:            self.anchors = np.array(anchors)        self.z_radius = z_radius        self.align_radius = align_radius        self.depth_thresh = depth_thresh        self.pred_vis = pred_vis        self.log = logging.getLogger(self.__class__.__name__)    def __getitem__(self, index):        """        Args:            index (int): Index        Returns:            tuple: Tuple (image, target). target is the object returned by ``itop.loadAnns``.        """        """        part 1: composition of depth image            Random insert foreground depth for humans captured from a list of positions            Random pick a background image for augmentation        """        image = np.ones([self.h_org, self.w_org]) * 2 * depth_max        fg_union = np.zeros([self.h_org, self.w_org])        anns = []        mod_id = random.randint(0, len(aug_mods)-1)        # print(aug_mods[mod_id])        for ii in aug_mods[mod_id]:            if uniform(0, 1) > 0.8:                continue            image_id = self.ids_list[ii][index % len(self.ids_list[ii])]            anns += copy.deepcopy(self.anno_dic_list[ii][image_id])            image_ii = np.load(os.path.join(self.img_dir, image_id)).astype(np.float)            fg_mask_ii = np.load(os.path.join(self.seg_dir, image_id)).astype(np.float)            # z-buffer composition of fg depth maps            image[fg_mask_ii > 0] = np.minimum((image_ii * fg_mask_ii)[fg_mask_ii > 0], image[fg_mask_ii > 0])            fg_union = np.maximum(fg_union, fg_mask_ii)        # make sure at least one example is added        if len(anns) == 0:            ii = random.randint(0, len(self.ids_list)-1)            image_id = self.ids_list[ii][index % len(self.ids_list[ii])]            anns += copy.deepcopy(self.anno_dic_list[ii][image_id])            image_ii = np.load(os.path.join(self.img_dir, image_id)).astype(np.float)            fg_mask_ii = np.load(os.path.join(self.seg_dir, image_id)).astype(np.float)            # z-buffer composition of fg depth maps            image[fg_mask_ii > 0] = np.minimum((image_ii * fg_mask_ii)[fg_mask_ii > 0], image[fg_mask_ii > 0])            fg_union = np.maximum(fg_union, fg_mask_ii)        bg_id = index % self.num_bg_images        bg_image_path = os.path.join(self.bg_dir, self.bg_list[bg_id]["file_name"])        bg_image = np.load(bg_image_path)        # compose fg bg depth map using fg mask        image = image * fg_union + bg_image * (np.ones_like(fg_union) - fg_union)        """        part 2: standard data augmentation        """        image, anns = self.preprocess((image, anns))        # data normalization        image[image < 0] = 0        image[image > depth_max] = depth_max        depth_resize = cv2.resize(image,                                  (np.int(self.input_x/self.strideZ), np.int(self.input_y/self.strideZ)))        image = self.image_transform(image)        """        part 3: data preparation for the ground-truth of specific dense map representations        """        heatmaps, pafs, zmaps, fg_masks_z, align_maps, fg_masks_align, prior_map, prior_mask_conf, prior_mask_coord, prior_weight_map = \            self.single_image_processing(anns, depth_resize)        return image, heatmaps, pafs, zmaps, fg_masks_z, align_maps, fg_masks_align, prior_map, prior_mask_conf, prior_mask_coord, prior_weight_map, index    def single_image_processing(self, anns, depth_resize):        """        :param anns: 2d and 3d annotations        :param depth_resize: it is a copy of image, but in numpy format for convenience        :return:        """        # prepare target maps        heatmaps, pafs, zmaps, fg_masks_z, align_maps, fg_masks_align,\        prior_map, prior_mask_conf, prior_mask_coord, prior_weight_map = \            self.get_ground_truth(anns, depth_resize)                heatmaps = torch.from_numpy(heatmaps.transpose((2, 0, 1)).astype(np.float32))        pafs = torch.from_numpy(pafs.transpose((2, 0, 1)).astype(np.float32))        zmaps = torch.from_numpy(zmaps.transpose((2, 0, 1)).astype(np.float32))        fg_masks_z = torch.from_numpy(fg_masks_z.transpose((2, 0, 1)).astype(np.float32))        align_maps = torch.from_numpy(align_maps.transpose((2, 0, 1)).astype(np.float32))        fg_masks_align = torch.from_numpy(fg_masks_align.transpose((2, 0, 1)).astype(np.float32))        prior_map = torch.from_numpy(prior_map.transpose((2, 0, 1)).astype(np.float32))        prior_mask_conf = torch.from_numpy(prior_mask_conf.transpose((2, 0, 1)).astype(np.float32))        prior_mask_coord = torch.from_numpy(prior_mask_coord.transpose((2, 0, 1)).astype(np.float32))        prior_weight_map = torch.from_numpy(prior_weight_map.transpose((2, 0, 1)).astype(np.float32))        return heatmaps, pafs, zmaps, fg_masks_z, align_maps, fg_masks_align, prior_map, prior_mask_conf, prior_mask_coord, prior_weight_map    def remove_illegal_joint(self, keypoints_2d, visibility=None):        if visibility is None:            visibility = np.ones([keypoints_2d.shape[0], keypoints_2d.shape[1]])        mask = np.logical_or.reduce((keypoints_2d[:, :, 0] >= self.input_x,                                     keypoints_2d[:, :,  0] < 0,                                     keypoints_2d[:, :, 1] >= self.input_y,                                     keypoints_2d[:, :, 1] < 0))        visibility[mask] = 0        return visibility    def build_prior_targets(self, ground_truth, weights, height, width, noobject_scale=0.1,                 object_scale=1.0):        """        This is the function to prepare anchor based pose representation for the prior subnetwork        ATTENTION: confidence mask needs to consider bg, but coordinate regression should only focus on fg.        Args:            ground_truth:            weights:            height: res of the prior map            width: res of the prior map        Returns:        """        num_anchors = self.anchors.shape[0]        # gt prior map that consistent with network output        gt_prior_map = np.zeros((height, width, num_anchors, (5 + self.num_joints * 3)))        # conf inference considers both bg and fg        fg_mask_conf = np.ones((height, width, num_anchors)) * noobject_scale        # coord inference only focuses on foreground        fg_mask_coord = np.zeros((height, width, num_anchors))        # consider bg as 1 (not its role). differentiable fg pose weights        gt_weight_map = np.ones((height, width, num_anchors))        anchors = np.concatenate([np.zeros_like(self.anchors), self.anchors], 1)        gt_box = np.zeros((len(ground_truth), 4))        # Need to convert ground-truth bbox into [xmin, ymin, h, w] format        for i, anno in enumerate(ground_truth):            gt_box[i, 0] = (anno[0] + anno[2]) / 2 / self.stride_prior            gt_box[i, 1] = (anno[1] + anno[3]) / 2 / self.stride_prior            gt_box[i, 2] = (anno[2] - anno[0]) / self.stride_prior            gt_box[i, 3] = (anno[3] - anno[1]) / self.stride_prior            anno[5:5 + 2 * self.num_joints] /= self.stride_prior        # Find best anchor for each ground truth        # Only rely on the box size to determine the assignment        gt_wh = np.copy(gt_box)        gt_wh[:, :2] = 0        iou_gt_anchors = self.bbox_ious(gt_wh, anchors)        best_anchors = np.argmax(iou_gt_anchors, axis=1)        # Set masks and target values for each ground truth        for i, anno in enumerate(ground_truth):            best_n = best_anchors[i]            gi = min(width - 1, max(0, int(gt_box[i, 0])))            gj = min(height - 1, max(0, int(gt_box[i, 1])))            fg_mask_conf[gj, gi, best_n] = object_scale            fg_mask_coord[gj, gi, best_n] = 1            gt_weight_map[gj, gi, :] = weights[i]            gt_prior_map[gj, gi, best_n, 0] = gt_box[i, 0] - gi            gt_prior_map[gj, gi, best_n, 1] = gt_box[i, 1] - gj            gt_prior_map[gj, gi, best_n, 2] = gt_box[i, 2] / self.anchors[best_n, 0]            gt_prior_map[gj, gi, best_n, 3] = gt_box[i, 3] / self.anchors[best_n, 1]            gt_prior_map[gj, gi, best_n, 4] = 1  # TODO: should use inbound ratio of gt box vs image for truncation case ??            # compute x,y coord canonical to current center            gt_prior_map[gj, gi, best_n, 5:] = anno[5:]            gt_prior_map[gj, gi, best_n, 5:5 + self.num_joints] -= gi            gt_prior_map[gj, gi, best_n, 5 + self.num_joints:5 + 2 * self.num_joints] -= gj            # normalize differently with different anchor sizes.            gt_prior_map[gj, gi, best_n, 5:5 + self.num_joints] /= (self.anchors[best_n, 0] / 2)            gt_prior_map[gj, gi, best_n, 5 + self.num_joints:5 + 2 * self.num_joints] /= (self.anchors[best_n, 1] / 2)        gt_prior_map = gt_prior_map.reshape((height, width, -1))        return gt_prior_map, fg_mask_conf, fg_mask_coord, gt_weight_map    def build_prior_targets_infer_visibility(self, ground_truth, weights, height, width, zmap, depth_thresh=0.03,                                             noobject_scale=0.1, object_scale=1.0):        """        This is the function to prepare anchor based pose representation for the prior subnetwork        ATTENTION: confidence mask needs to consider bg, but coordinate regression should only focus on fg.        Args:            ground_truth:            weights:            height: res of the prior map            width: res of the prior map        Returns:        """        # infer uncertainty from the prior-zmap disparity        Zgrid_h = int(self.input_y / self.strideZ)        Zgrid_w = int(self.input_x / self.strideZ)        ground_truth_new = []        for i, anno in enumerate(ground_truth):            visibility = np.ones(self.num_joints)            for j in range(self.num_joints):                x_j = int(anno[5 + j] / self.strideZ)                y_j = int(anno[5 + self.num_joints + j] / self.strideZ)                d_j = anno[5 + self.num_joints*2 + j]                if x_j < 0 or x_j >= Zgrid_w or y_j < 0 or y_j >= Zgrid_h:                    visibility[j] = 0                    continue                if abs(zmap[y_j, x_j, j] - d_j) * depth_std > depth_thresh:                    visibility[j] = 0            anno_new = np.concatenate([anno, visibility])            ground_truth_new.append(anno_new)        ground_truth = ground_truth_new        num_anchors = self.anchors.shape[0]        # gt prior map that consistent with network output        gt_prior_map = np.zeros((height, width, num_anchors, (5 + self.num_joints * 4)))        # conf inference considers both bg and fg        fg_mask_conf = np.ones((height, width, num_anchors)) * noobject_scale        # coord inference only focuses on foreground        fg_mask_coord = np.zeros((height, width, num_anchors))        # consider bg as 1 (not its role). differentiable fg pose weights        gt_weight_map = np.ones((height, width, num_anchors))        anchors = np.concatenate([np.zeros_like(self.anchors), self.anchors], 1)        gt_box = np.zeros((len(ground_truth), 4))        # Need to convert ground-truth bbox into [xmin, ymin, h, w] format        for i, anno in enumerate(ground_truth):            gt_box[i, 0] = (anno[0] + anno[2]) / 2 / self.stride_prior            gt_box[i, 1] = (anno[1] + anno[3]) / 2 / self.stride_prior            gt_box[i, 2] = (anno[2] - anno[0]) / self.stride_prior            gt_box[i, 3] = (anno[3] - anno[1]) / self.stride_prior            anno[5:5 + 2 * self.num_joints] /= self.stride_prior        # Find best anchor for each ground truth        # Only rely on the box size to determine the assignment        gt_wh = np.copy(gt_box)        gt_wh[:, :2] = 0        iou_gt_anchors = self.bbox_ious(gt_wh, anchors)        best_anchors = np.argmax(iou_gt_anchors, axis=1)        # Set masks and target values for each ground truth        for i, anno in enumerate(ground_truth):            best_n = best_anchors[i]            gi = min(width - 1, max(0, int(gt_box[i, 0])))            gj = min(height - 1, max(0, int(gt_box[i, 1])))            fg_mask_conf[gj, gi, best_n] = object_scale            fg_mask_coord[gj, gi, best_n] = 1            gt_weight_map[gj, gi, :] = weights[i]            gt_prior_map[gj, gi, best_n, 0] = gt_box[i, 0] - gi            gt_prior_map[gj, gi, best_n, 1] = gt_box[i, 1] - gj            gt_prior_map[gj, gi, best_n, 2] = gt_box[i, 2] / self.anchors[best_n, 0]            gt_prior_map[gj, gi, best_n, 3] = gt_box[i, 3] / self.anchors[best_n, 1]            gt_prior_map[gj, gi, best_n, 4] = 1  # TODO: should use inbound ratio of gt box vs image for truncation case ??            # compute x,y coord canonical to current center            gt_prior_map[gj, gi, best_n, 5:] = anno[5:]            gt_prior_map[gj, gi, best_n, 5:5 + self.num_joints] -= gi            gt_prior_map[gj, gi, best_n, 5 + self.num_joints:5 + 2 * self.num_joints] -= gj            # normalize differently with different anchor sizes.            gt_prior_map[gj, gi, best_n, 5:5 + self.num_joints] /= (self.anchors[best_n, 0] / 2)            gt_prior_map[gj, gi, best_n, 5 + self.num_joints:5 + 2 * self.num_joints] /= (self.anchors[best_n, 1] / 2)        gt_prior_map = gt_prior_map.reshape((height, width, -1))        return gt_prior_map, fg_mask_conf, fg_mask_coord, gt_weight_map    def bbox_ious(self, boxes1, boxes2):        b1x1, b1y1 = np.hsplit(boxes1[:, :2] - (boxes1[:, 2:4] / 2), 2)        b1x2, b1y2 = np.hsplit(boxes1[:, :2] + (boxes1[:, 2:4] / 2), 2)        b2x1, b2y1 = np.hsplit(boxes2[:, :2] - (boxes2[:, 2:4] / 2), 2)        b2x2, b2y2 = np.hsplit(boxes2[:, :2] + (boxes2[:, 2:4] / 2), 2)        # broadcast        b1x1_cast = np.tile(b1x1.reshape(-1, 1), (1, len(b2x1)))        b1y1_cast = np.tile(b1y1.reshape(-1, 1), (1, len(b2y1)))        b1x2_cast = np.tile(b1x2.reshape(-1, 1), (1, len(b2x2)))        b1y2_cast = np.tile(b1y2.reshape(-1, 1), (1, len(b2y2)))        b2x1_cast = np.tile(b2x1.reshape(1, -1), (len(b1x1), 1))        b2y1_cast = np.tile(b2y1.reshape(1, -1), (len(b1y1), 1))        b2x2_cast = np.tile(b2x2.reshape(1, -1), (len(b1x2), 1))        b2y2_cast = np.tile(b2y2.reshape(1, -1), (len(b1y2), 1))        dx = np.minimum(b1x2_cast, b2x2_cast) - np.maximum(b1x1_cast, b2x1_cast)        dy = np.minimum(b1y2_cast, b2y2_cast) - np.maximum(b1y1_cast, b2y1_cast)        dx[dx < 0] = 0        dy[dy < 0] = 0        intersections = dx * dy        areas1 = (b1x2_cast - b1x1_cast) * (b1y2_cast - b1y1_cast)        areas2 = (b2x2_cast - b2x1_cast) * (b2y2_cast - b2y1_cast)        unions = (areas1 + areas2) - intersections        return intersections / unions    # This is the key function to prepare a set of target maps    def get_ground_truth(self, anns, depth_input):        grid_h = int(self.input_y / self.stride)        grid_w = int(self.input_x / self.stride)        Zgrid_h = int(self.input_y / self.strideZ)        Zgrid_w = int(self.input_x / self.strideZ)        Agrid_h = int(self.input_y / self.strideA)        Agrid_w = int(self.input_x / self.strideA)        prior_h = int(self.input_y / self.stride_prior)        prior_w = int(self.input_x / self.stride_prior)        heatmaps = np.zeros((int(grid_h), int(grid_w), self.num_joints + 1))        pafs = np.zeros((int(grid_h), int(grid_w), 2 * len(self.limb_ids)))        # ATTENTION: initialize with max and only work on foreground. At last update background with the union fg mask        # initialize pose depth with input depth map        zmaps_org = np.repeat(depth_input[:, :, np.newaxis], len(self.joint_names), axis=2)        zmaps = np.ones_like(zmaps_org) * 2*depth_max        fg_masks_z = np.zeros((int(Zgrid_h), int(Zgrid_w), len(self.joint_names)))        align_maps = np.zeros((int(Agrid_h), int(Agrid_w), 2*len(self.joint_names)))        fg_masks_align = np.zeros((int(Agrid_h), int(Agrid_w), 2*len(self.joint_names)))        max_dist = 2 * (self.align_radius+0.5)        dist_maps = np.ones((int(Agrid_h), int(Agrid_w), len(self.joint_names))).astype(np.float32)*max_dist        # This considers multiple annotations per frame later        keypoints_2d = []        keypoints_3d = []        pose_weights = []        objects = []        for ann in anns:            keypoints_2d.append(np.array(ann['2d_joints']).reshape(-1, 2))            keypoints_3d.append(np.array(ann['3d_joints']).reshape(-1, 3))            # # this is a verification of data augmentation error            # single_2d = np.array(ann['2d_joints']).reshape(-1, 2)            # single_3d = np.array(ann['3d_joints']).reshape(-1, 3)            # single_3d_compute = pos_3d_from_2d_and_depth(single_2d[:, 0] / self.input_x * self.w_org, single_2d[:, 1] / self.input_y * self.h_org, single_3d[:, 2], intrinsics['cx'], intrinsics['cy'], intrinsics['fx'], intrinsics['fy'])            # print(single_3d_compute - single_3d)            pose_weights.append(ann['pose_weight'])            object = np.array([ann['bbox'][0], ann['bbox'][1],                               ann['bbox'][2], ann['bbox'][3], 1.0])            object = np.concatenate([object, ann['2d_joints'][:, 0].ravel()])            object = np.concatenate([object, ann['2d_joints'][:, 1].ravel()])            object = np.concatenate([object, ann['3d_joints'][:, 2].ravel()])            object[5 + 2 * self.num_joints: 5 + 3 * self.num_joints] -= depth_mean            object[5 + 2 * self.num_joints: 5 + 3 * self.num_joints] /= depth_std            objects.append(object)        keypoints_2d = np.array(keypoints_2d)        inbounds = self.remove_illegal_joint(keypoints_2d)  # indicate inbound of image        keypoints_3d = np.array(keypoints_3d)        # pose_weights = np.array([pose_weights[0]])        objects = np.array(objects)        """            Notes:                Truncation and occlusion should be considered separately                Occluded part needs to be prepared in regression maps so that they can be predicted in its channel.                This is ambiguity in multi-person overlapping of the same joint type.                So it require to label the occlusion caused by same joint type or others.        """        # confidence maps for body parts        for i in range(self.num_joints):            joints = keypoints_2d[:, i, :]            for j, joint in enumerate(joints):                # consider visible and within-range joint                if inbounds[j, i] > 0.5:                    root_center = joint[:2]                    gaussian_map = heatmaps[:, :, i]                    heatmaps[:, :, i] = putGaussianMaps(                        root_center, gaussian_map,                        7.0, grid_h, grid_w, self.stride)        # background: the last dimension of the heatmap prepared for background. Default for cross-entropy loss        heatmaps[:, :, -1] = np.maximum(            1 - np.max(heatmaps[:, :, :self.num_joints], axis=2),            0.        )        # pafs for limbs        for i, (k1, k2) in enumerate(self.limb_ids):            # limb            count = np.zeros((int(grid_h), int(grid_w)), dtype=np.uint32)            for j, joints in enumerate(keypoints_2d):                if inbounds[j, k1] > 0.5 and inbounds[j, k2] > 0.5:                    centerA = joints[k1, :2]                    centerB = joints[k2, :2]                    vec_map = pafs[:, :, 2 * i:2 * (i + 1)]                    pafs[:, :, 2 * i:2 * (i + 1)], count = putVecMaps(                        centerA=centerA,                        centerB=centerB,                        accumulate_vec_map=vec_map,                        count=count, grid_y=grid_h, grid_x=grid_w, stride=self.stride                    )        # Z maps        for j, joints in enumerate(keypoints_2d):            for k, joint in enumerate(joints):                if inbounds[j, k] < 0.5:                    continue                joint_center = joint[:2]                joint_depth = keypoints_3d[j, k, 2]                map_z = zmaps[:, :, k]                fg_mask_z = fg_masks_z[:, :, k]                zmaps[:, :, k], fg_masks_z[:, :, k] = putJointZ(                    center=joint_center,                    depth=joint_depth,                    accumulate_map=map_z,                    accumulate_mask=fg_mask_z,                    grid_y=Zgrid_h, grid_x=Zgrid_w, stride=self.strideZ, radius=self.z_radius, max_depth=2*depth_max                )        # align maps        for j, joints in enumerate(keypoints_2d):            for k, joint in enumerate(joints):                if inbounds[j, k] < 0.5:                    continue                joint_center = joint[:2]                joint_depth = keypoints_3d[j, k, 2]                fg_mask_align = fg_masks_align[:, :, 2 * k: 2 * (k + 1)]                mapDxDy = align_maps[:, :, 2 * k: 2 * (k + 1)]                align_maps[:, :, 2 * k: 2 * (k + 1)],\                fg_masks_align[:, :, 2 * k: 2 * (k + 1)],\                dist_maps[:, :, k] = putJointDxDy(                    center=joint_center,                    depth=joint_depth,                    accumulate_map=mapDxDy,                    accumulate_mask=fg_mask_align,                    accumulate_dist=dist_maps[:, :, k],                    z_map=zmaps[:, :, k],                    grid_y=Agrid_h, grid_x=Agrid_w, stride=self.strideA, radius=self.align_radius, max_dist=max_dist                )        # apply zmap bg        zmaps[fg_masks_z == 0] = zmaps_org[fg_masks_z == 0]        # normalize Z map        zmaps[zmaps < 0] = 0        zmaps[zmaps > depth_max] = depth_max        zmaps -= depth_mean        zmaps /= depth_std        if self.pred_vis:            # prepare prior map and associated masks and weights            prior_map, prior_mask_conf, prior_mask_coord, prior_weight_map = \                self.build_prior_targets_infer_visibility(objects, pose_weights, prior_h, prior_w, zmaps,                                                          depth_thresh=self.depth_thresh)        else:            # prepare prior map and associated masks and weights            prior_map, prior_mask_conf, prior_mask_coord, prior_weight_map = \                self.build_prior_targets(objects, pose_weights, prior_h, prior_w)        return heatmaps, pafs, zmaps, fg_masks_z, align_maps, fg_masks_align, prior_map, prior_mask_conf, prior_mask_coord, prior_weight_map    def __len__(self):        return self.dataset_len    def generate_test_batch(self, batch_size):        """        Because the labels are online generated and there are no saved one to use in evaluation, it is necessary to        use this separate function to output labels with undetermined number of instances        Args:        Returns:            images in numpy arrays            labels in list        """        """        part 1: composition of depth image            Random insert foreground depth for humans captured from a list of positions            Random pick a background image for augmentation        """        images = []        labels = []        for b in range(batch_size):            # index needs to be random            index = random.randint(0, self.dataset_len - 1)            image = np.ones([self.h_org, self.w_org]) * 2 * depth_max            fg_union = np.zeros([self.h_org, self.w_org])            anns = []            mod_id = random.randint(0, len(aug_mods)-1)            # print(aug_mods[mod_id])            for ii in aug_mods[mod_id]:                if uniform(0, 1) > 0.8:                    continue                image_id = self.ids_list[ii][index % len(self.ids_list[ii])]                anns += copy.deepcopy(self.anno_dic_list[ii][image_id])                image_ii = np.load(os.path.join(self.img_dir, image_id)).astype(np.float)                fg_mask_ii = np.load(os.path.join(self.seg_dir, image_id)).astype(np.float)                # z-buffer composition of fg depth maps                image[fg_mask_ii > 0] = np.minimum((image_ii * fg_mask_ii)[fg_mask_ii > 0], image[fg_mask_ii > 0])                fg_union = np.maximum(fg_union, fg_mask_ii)            # make sure at least one example is added            if len(anns) == 0:                ii = random.randint(0, len(self.ids_list)-1)                image_id = self.ids_list[ii][index % len(self.ids_list[ii])]                anns += copy.deepcopy(self.anno_dic_list[ii][image_id])                image_ii = np.load(os.path.join(self.img_dir, image_id)).astype(np.float)                fg_mask_ii = np.load(os.path.join(self.seg_dir, image_id)).astype(np.float)                # z-buffer composition of fg depth maps                image[fg_mask_ii > 0] = np.minimum((image_ii * fg_mask_ii)[fg_mask_ii > 0], image[fg_mask_ii > 0])                fg_union = np.maximum(fg_union, fg_mask_ii)            bg_id = index % self.num_bg_images            bg_image_path = os.path.join(self.bg_dir, self.bg_list[bg_id]["file_name"])            bg_image = np.load(bg_image_path)            # compose fg bg depth map using fg mask            image = image * fg_union + bg_image * (np.ones_like(fg_union) - fg_union)            """            part 2: only the resize process, the output annos should use the original             """            anns_new = np.copy(anns)            image, anns_new = self.preprocess((image, anns_new))            # data normalization            image[image < 0] = 0            image[image > depth_max] = depth_max            image -= depth_mean            image /= depth_std            images.append(image)            # labels.append(anns)            labels.append(anns_new)        return np.array(images), labels# unit test for dataloaderif __name__ == "__main__":    vis = True    w_org = 480    h_org = 512    input_size = 224    num_joints = 15    num_bones = 14    dim_reduction = 8    dim_reductionZ = 8    dim_reductionA = 8    batch_size = 5    ZRadius = 2    AlignRadius = 2    pred_vis = True    anchors = [(6., 3.), (12., 6.)]    max_ratio = 1.5    joint2chn = np.array(range(num_joints))    DATA_DIR = '/media/yuliang/DATA/Datasets/Kinect_Depth_Human3D'    img_dir = os.path.join(DATA_DIR, 'depth_maps')    ANNOTATIONS_TRAIN = sorted(glob('{}/labels_train_*.json'.format(os.path.join(DATA_DIR, 'labels'))))    # ANNOTATIONS_TRAIN = [os.path.join(DATA_DIR, 'labels', item) for item in ['labels_train_*.json']]    bg_file = os.path.join(DATA_DIR, 'labels', 'labels_bg.json')    bg_dir = os.path.join(DATA_DIR, 'bg_maps')    seg_dir = os.path.join(DATA_DIR, 'seg_maps')    preprocess = data_augmentation_2d3d.Compose([        data_augmentation_2d3d.Cvt2ndarray(),        data_augmentation_2d3d.Rotate(cx=intrinsics['cx'], cy=intrinsics['cy']),        data_augmentation_2d3d.RenderDepth(cx=intrinsics['cx'], cy=intrinsics['cy'], max_ratio=max_ratio),        # data_augmentation_depth_3d.Hflip(swap_indices=get_swap_part_indices()),        data_augmentation_2d3d.Crop(),  # it may violate the 3D-2D geometry        data_augmentation_2d3d.Resize(input_size)    ])    train_data = KDH3D_Keypoints(        img_dir=img_dir,        ann_file_list=ANNOTATIONS_TRAIN,        preprocess=preprocess,        h_org=h_org,        w_org=w_org,        input_x=input_size,        input_y=input_size,        stride=dim_reduction,        stride_z=dim_reductionZ,        stride_a=dim_reductionA,        z_radius=ZRadius,        align_radius=AlignRadius,        anchors=anchors,        bg_file=bg_file,        bg_dir=bg_dir,        seg_dir=seg_dir,        pred_vis=pred_vis    )    train_loader = torch.utils.data.DataLoader(        train_data, batch_size=batch_size, shuffle=True,        pin_memory=True, num_workers=0, drop_last=True)  # set num_wrkers=1 for debugging    cnt = 0    for i, (img, heatmap_target, paf_target, zmap_target, fg_masks_z, alignmap_target, fg_masks_align, prior_map, prior_mask_conf, prior_mask_coord, prior_weight_map, index) in enumerate(train_loader):        # verify 2D pos + Z branch recovers 3D pose,        heatmap_target = heatmap_target.cpu().data.numpy().transpose(0, 2, 3, 1)        zmap_target = zmap_target.cpu().data.numpy().transpose(0, 2, 3, 1)        zmap_target[:, :, :, :num_joints] *= depth_std        zmap_target[:, :, :, :num_joints] += depth_mean        alignmap_target = alignmap_target.cpu().data.numpy().transpose(0, 2, 3, 1)        alignmap_target = alignmap_target * (AlignRadius + 0.5)        # prior_map = prior_map.cpu().data.numpy().transpose(0, 2, 3, 1)        prior_map = prior_map.cuda()        # evaluate the and visualize direct prior results        bboxes, humans_prior, visibilities = parse_prior_pose(prior_map,                                                              anchors,                                                              num_joints,                                                              input_size,                                                              input_size,                                                              depth_mean,                                                              depth_std,                                                              conf_threshold=0.5,                                                              nms_threshold=0.6,                                                              pred_vis=pred_vis)        if vis is True:            for b in range(batch_size):                single_img = img[b, 0, :, :].numpy()                single_heatmap = heatmap_target[b, ...]                single_paf = paf_target[b, ...].numpy()                single_zmap = zmap_target[b, ...]                single_alignmap = alignmap_target[b, ...]                """                self-validation between gt prior and local map read                """                humans_2d_gt = [humans_prior[b][hi][:, :2] for hi in range(len(humans_prior[b]))]                humans_depth_gt = [humans_prior[b][hi][:, 2] for hi in range(len(humans_prior[b]))]                visibility = [visibilities[b][hi].astype(np.float) for hi in range(len(visibilities[b]))]                dists_2d = np.ones((len(humans_2d_gt), num_joints)) * (-1)                dists_depth = np.ones((len(humans_2d_gt), num_joints)) * (-1)                for i, human_gt in enumerate(humans_2d_gt):                    print('person: {:d}'.format(i))                    for j, joint in enumerate(human_gt):                        if visibility[i][j] > 0.5:                            x2d = int(joint[0] / dim_reduction)                            y2d = int(joint[1] / dim_reduction)                            # guide the prior joint to the local map center                            dx, dy = retrieve_offsets_direct([x2d, y2d], single_alignmap[:, :, 2 * joint2chn[j]:2 * joint2chn[j] + 2])                            x2d_aligned = (float(x2d) + dx) * dim_reduction                            y2d_aligned = (float(y2d) + dy) * dim_reduction                            dists_2d[i, j] = np.sqrt((x2d_aligned-joint[0])**2 + (y2d_aligned-joint[1])**2)                            # dists_2d[i, j] = (y2d_aligned-joint[1])                            # ATTENTION, using the after-align location can be sensitive to align field correctness                            # x2d = int(float(x2d) + dx)                            # y2d = int(float(y2d) + dy)                            depth_read = single_zmap[y2d, x2d, joint2chn[j]]                            # depth_read = retrieve_depth_heat_weighted(                            #     [x2d, y2d],                            #     single_zmap[:, :, joint2chn[j]],                            #     single_heatmap[:, :, joint2chn[j]], radius=1)                            depth_read = depth_read * depth_std + depth_mean                            depth_gt = humans_depth_gt[i][j] * depth_std + depth_mean                            dists_depth[i, j] = abs(depth_read-depth_gt)                    for j, name in enumerate(get_keypoints()):                        print('     joint: {},  2d error: {:03f}'.format(name, dists_2d[i, j]))                    print('\n')                    for j, name in enumerate(get_keypoints()):                        print('     joint: {},  d error: {:03f}'.format(name, dists_depth[i, j]))                    print('\n')                    for j, name in enumerate(get_keypoints()):                        print('     joint: {},  visibility: {:03f}'.format(name, visibility[i][j]))                    print('\n')                # normalize depth image for visualization                single_img *= depth_std                single_img += depth_mean                single_img[single_img <= 0] = 0                single_img[single_img >= depth_max] = depth_max                single_img /= depth_max                fig1 = plt.figure()                ax1 = fig1.add_subplot(241)                ax1.imshow(single_img)                ax1.set_title('depth input')                # visualize heatmap                ht_max = np.max(single_heatmap[:, :, :-1], axis=2)                ax2 = fig1.add_subplot(242)                ax2.imshow(ht_max)                ax2.set_title('ht max')                # visualize paf map                paf_x = single_paf[0:2*num_bones:2, ...]                paf_x = np.sum(paf_x, axis=0)                paf_y = single_paf[1:2*num_bones:2, ...]                paf_y = np.sum(paf_y, axis=0)                ax3 = fig1.add_subplot(243)                ax3.imshow(paf_x)                ax3.set_title('paf x')                ax4 = fig1.add_subplot(247)                ax4.imshow(paf_y)                ax4.set_title('paf y')                # visualize zmap                # ATTENTION: simply visualizing min may keep the noisy part.                zmap_vis = np.min(single_zmap, axis=2)                zmap_vis[zmap_vis <= 0] = 0                zmap_vis[zmap_vis >= depth_max] = depth_max                zmap_vis /= depth_max                ax5 = fig1.add_subplot(245)                ax5.imshow(zmap_vis)                ax5.set_title('poseD head')                # draw humans on the image                single_img *= 255                single_img = cv2.cvtColor(single_img.astype(np.uint8), cv2.COLOR_GRAY2BGR)                # for j in range(len(bboxes[b])):                #     single_bbox = bboxes[b][j]                #     cv2.rectangle(single_img, (int(single_bbox[0]), int(single_bbox[1])), (int(single_bbox[2]), int(single_bbox[3])), [0, 255, 0], 2)                single_img = draw_humans_visibility(single_img,                                                    humans_2d_gt,                                                    kp_connections(get_keypoints()),                                                    jointColors)                save_name = '{:08d}.png'.format(cnt)                cv2.imwrite(save_name, single_img)                single_img = np.flip(single_img, 2)                ax6 = fig1.add_subplot(246)                ax6.imshow(single_img)                ax6.set_title('skeleton')                # visualize dx dy field maps, using sum for simplicity, but not correct for occlusion                dx_field = single_alignmap[:, :, 0: 2 * num_joints:2]                dx_field = np.sum(dx_field, axis=2)                dy_field = single_alignmap[:, :, 1: 2 * num_joints:2]                dy_field = np.sum(dy_field, axis=2)                ax7 = fig1.add_subplot(244)                ax7.imshow(dx_field)                ax7.set_title('dx field')                ax8 = fig1.add_subplot(248)                ax8.imshow(dy_field)                ax8.set_title('dy field')                fig1.subplots_adjust(wspace=0.1, hspace=0.01)                # print('pose weight: {}'.format(pose_weights[b]))                fig1.show()                plt.waitforbuttonpress()                plt.pause(1)                plt.close(fig1)        cnt += 1        print('iter: [{}/{}]\t'.format(i, len(train_loader)))