"""Data Preparation for generic 3d pose align network on ITOP dataset"""import copyimport loggingimport osimport numpy as npimport jsonimport matplotlib.pyplot as pltimport cv2import randomfrom random import uniformfrom glob import globimport torch.utils.dataimport torchvisionfrom PIL import Imageimport flow_visfrom lib.datasets.heatmap import putGaussianMapsfrom lib.datasets.paf import putVecMapsfrom lib.datasets.posemap import putJointZ, putJointDxDyfrom lib.datasets import data_augmentation_2d3dfrom lib.utils.common import draw_humans, draw_humans_visibility, superimpose_colormap_on_imgfrom lib.utils.prior_pose_align import parse_prior_pose_rgbplt.rcParams['figure.figsize'] = (9, 6)jointColors = [[255, 0, 0], [255, 85, 0], [255, 170, 0], [255, 255, 0], [170, 255, 0], [85, 255, 0], [0, 255, 0],               [0, 255, 85], [0, 255, 170], [0, 255, 255], [0, 170, 255], [0, 85, 255], [0, 0, 255], [85, 0, 255],               [170, 0, 255], [255, 0, 255], [255, 0, 170], [255, 0, 85], [85, 255, 85]]root_joint = 'PELVIS'joint2box_margin = 10bgr_mean = [0.406, 0.456, 0.485]bgr_std = [0.225, 0.224, 0.229]rgb_mean = [0.485, 0.456, 0.406]rgb_std = [0.229, 0.224, 0.225]def kp_connections(keypoints):    kp_lines = [        [keypoints.index('PELVIS'), keypoints.index('HIP_RIGHT')],        [keypoints.index('HIP_RIGHT'), keypoints.index('KNEE_RIGHT')],        [keypoints.index('KNEE_RIGHT'), keypoints.index('ANKLE_RIGHT')],        [keypoints.index('PELVIS'), keypoints.index('HIP_LEFT')],        [keypoints.index('HIP_LEFT'), keypoints.index('KNEE_LEFT')],        [keypoints.index('KNEE_LEFT'), keypoints.index('ANKLE_LEFT')],        [keypoints.index('PELVIS'), keypoints.index('THORAX')],        [keypoints.index('UPPER_NECK'), keypoints.index('SHOULDER_RIGHT')],        [keypoints.index('SHOULDER_RIGHT'), keypoints.index('ELBOW_RIGHT')],        [keypoints.index('ELBOW_RIGHT'), keypoints.index('WRIST_RIGHT')],        [keypoints.index('UPPER_NECK'), keypoints.index('SHOULDER_LEFT')],        [keypoints.index('SHOULDER_LEFT'), keypoints.index('ELBOW_LEFT')],        [keypoints.index('ELBOW_LEFT'), keypoints.index('WRIST_LEFT')],        [keypoints.index('THORAX'), keypoints.index('UPPER_NECK')],        [keypoints.index('UPPER_NECK'), keypoints.index('HEAD_TOP')]    ]    return kp_linesdef get_keypoints():    """Get the itop keypoints"""    keypoints = [        'ANKLE_RIGHT',        'KNEE_RIGHT',        'HIP_RIGHT',        'HIP_LEFT',        'KNEE_LEFT',        'ANKLE_LEFT',        'PELVIS',        'THORAX',        'UPPER_NECK',        'HEAD_TOP',        'WRIST_RIGHT',        'ELBOW_RIGHT',        'SHOULDER_RIGHT',        'SHOULDER_LEFT',        'ELBOW_LEFT',        'WRIST_LEFT']    return keypointsdef get_swap_part_indices():    keypoints = get_keypoints()    swap_indices = []    for keypoint in keypoints:        if keypoint == 'SHOULDER_RIGHT':            swap_indices.append(keypoints.index('SHOULDER_LEFT'))            continue        if keypoint == 'SHOULDER_LEFT':            swap_indices.append(keypoints.index('SHOULDER_RIGHT'))            continue        if keypoint == 'ELBOW_RIGHT':            swap_indices.append(keypoints.index('ELBOW_LEFT'))            continue        if keypoint == 'ELBOW_LEFT':            swap_indices.append(keypoints.index('ELBOW_RIGHT'))            continue        if keypoint == 'WRIST_RIGHT':            swap_indices.append(keypoints.index('WRIST_LEFT'))            continue        if keypoint == 'WRIST_LEFT':            swap_indices.append(keypoints.index('WRIST_RIGHT'))            continue        if keypoint == 'HIP_RIGHT':            swap_indices.append(keypoints.index('HIP_LEFT'))            continue        if keypoint == 'HIP_LEFT':            swap_indices.append(keypoints.index('HIP_RIGHT'))            continue        if keypoint == 'KNEE_RIGHT':            swap_indices.append(keypoints.index('KNEE_LEFT'))            continue        if keypoint == 'KNEE_LEFT':            swap_indices.append(keypoints.index('KNEE_RIGHT'))            continue        if keypoint == 'ANKLE_RIGHT':            swap_indices.append(keypoints.index('ANKLE_LEFT'))            continue        if keypoint == 'ANKLE_LEFT':            swap_indices.append(keypoints.index('ANKLE_RIGHT'))            continue        swap_indices.append(keypoints.index(keypoint))    return swap_indicesclass MPII_Keypoints(torch.utils.data.Dataset):    """Kinect Depth Human 3D (KDH3D) Dataset.    Caches preprocessing.    Args:        img_dir (string): Root directory where images are saved.        ann_file (string): Path to json annotation file.        transform (callable, optional): A function/transform that  takes in an PIL image            and returns a transformed version. E.g, ``transforms.ToTensor``        target_transform (callable, optional): A function/transform that takes in the            target and transforms it.    """    def __init__(self, img_dir, ann_file, is_train=True, preprocess=None, input_x=368, input_y=368, stride=8, stride_a=None, stride_prior=None, anchors=None,  align_radius=3):        self.img_dir = img_dir        # load all the ids and joint annotations        self.anno_dic = json.load(open(ann_file, 'r'))        self.ids = [key for key, value in self.anno_dic.items()]        self.is_train = is_train        # preprocess for data augmentation, at original image values        self.preprocess = preprocess        # standard totensor and normalize, apply after data augmentation        self.image_transform = torchvision.transforms.Compose(            [torchvision.transforms.ToTensor(),             torchvision.transforms.Normalize(mean=rgb_mean, std=rgb_std)]        )        self.joint_names = get_keypoints()        self.limb_ids = kp_connections(self.joint_names)        self.root_id = self.joint_names.index(root_joint)        self.num_joints = len(self.joint_names)        self.input_x = input_x        self.input_y = input_y        self.stride = stride        if stride_a is None:            self.strideA = stride        else:            self.strideA = stride_a        if stride_prior is None:            self.stride_prior = stride * 2        else:            self.stride_prior = stride_prior        if anchors is None:            anchor_h = input_y / self.stride_prior - 3            self.anchors = np.array([(anchor_h/2, anchor_h/4), (anchor_h, anchor_h/2)])        else:            self.anchors = np.array(anchors)        self.align_radius = align_radius        self.log = logging.getLogger(self.__class__.__name__)    def __getitem__(self, index):        """        Args:            index (int): Index        Returns:            tuple: Tuple (image, target). target is the object returned by ``itop.loadAnns``.        """        """           part 1: standard data augmentation        """        image_id = self.ids[index]        anns = self.anno_dic[image_id]        anns = copy.deepcopy(anns)        image = cv2.imread(os.path.join(self.img_dir, image_id)).astype(np.float)/255        image = np.flip(image, 2)        """        part 2: standard data augmentation        """        image, anns = self.preprocess((image, anns))        # data normalization        image = self.image_transform(image)        if not self.is_train:            return image, index        """        part 3: data preparation for the ground-truth of specific dense map representations        """        heatmaps, align_maps, fg_masks_align, prior_map, prior_mask_conf, prior_mask_coord, prior_weight_map = \            self.single_image_processing(anns)        return image, heatmaps, align_maps, fg_masks_align, prior_map, prior_mask_conf, prior_mask_coord, prior_weight_map, index    def single_image_processing(self, anns):        """        :param anns: 2d and 3d annotations        :param depth_resize: it is a copy of image, but in numpy format for convenience        :return:        """        # prepare target maps        heatmaps, align_maps, fg_masks_align,\        prior_map, prior_mask_conf, prior_mask_coord, prior_weight_map = \            self.get_ground_truth(anns)                heatmaps = torch.from_numpy(heatmaps.transpose((2, 0, 1)).astype(np.float32))        align_maps = torch.from_numpy(align_maps.transpose((2, 0, 1)).astype(np.float32))        fg_masks_align = torch.from_numpy(fg_masks_align.transpose((2, 0, 1)).astype(np.float32))        prior_map = torch.from_numpy(prior_map.transpose((2, 0, 1)).astype(np.float32))        prior_mask_conf = torch.from_numpy(prior_mask_conf.transpose((2, 0, 1)).astype(np.float32))        prior_mask_coord = torch.from_numpy(prior_mask_coord.transpose((2, 0, 1)).astype(np.float32))        prior_weight_map = torch.from_numpy(prior_weight_map.transpose((2, 0, 1)).astype(np.float32))        return heatmaps, align_maps, fg_masks_align, prior_map, prior_mask_conf, prior_mask_coord, prior_weight_map    def remove_illegal_joint(self, keypoints_2d, visibility=None):        if visibility is None:            visibility = np.ones([keypoints_2d.shape[0], keypoints_2d.shape[1]])        mask = np.logical_or.reduce((keypoints_2d[:, :, 0] >= self.input_x,                                     keypoints_2d[:, :,  0] < 0,                                     keypoints_2d[:, :, 1] >= self.input_y,                                     keypoints_2d[:, :, 1] < 0))        visibility[mask] = 0        return visibility    def build_prior_targets(self, ground_truth, weights, height, width, noobject_scale=0.1,                 object_scale=1.0):        """        This is the function to prepare anchor based pose representation for the prior subnetwork        ATTENTION: confidence mask needs to consider bg, but coordinate regression should only focus on fg.        Args:            ground_truth:            weights:            height: res of the prior map            width: res of the prior map        Returns:        """        num_anchors = self.anchors.shape[0]        # gt prior map that consistent with network output        gt_prior_map = np.zeros((height, width, num_anchors, (5 + self.num_joints * 3)))        # conf inference considers both bg and fg        fg_mask_conf = np.ones((height, width, num_anchors)) * noobject_scale        # coord inference only focuses on foreground        fg_mask_coord = np.zeros((height, width, num_anchors))        # consider bg as 1 (not its role). differentiable fg pose weights        gt_weight_map = np.ones((height, width, num_anchors))        anchors = np.concatenate([np.zeros_like(self.anchors), self.anchors], 1)        gt_box = np.zeros((len(ground_truth), 4))        # Need to convert ground-truth bbox into [xmin, ymin, h, w] format        for i, anno in enumerate(ground_truth):            gt_box[i, 0] = (anno[0] + anno[2]) / 2 / self.stride_prior            gt_box[i, 1] = (anno[1] + anno[3]) / 2 / self.stride_prior            gt_box[i, 2] = (anno[2] - anno[0]) / self.stride_prior            gt_box[i, 3] = (anno[3] - anno[1]) / self.stride_prior            anno[5:5 + 2 * self.num_joints] /= self.stride_prior        # Find best anchor for each ground truth        # Only rely on the box size to determine the assignment        gt_wh = np.copy(gt_box)        gt_wh[:, :2] = 0        iou_gt_anchors = self.bbox_ious(gt_wh, anchors)        best_anchors = np.argmax(iou_gt_anchors, axis=1)        # Set masks and target values for each ground truth        for i, anno in enumerate(ground_truth):            best_n = best_anchors[i]            gi = min(width - 1, max(0, int(gt_box[i, 0])))            gj = min(height - 1, max(0, int(gt_box[i, 1])))            fg_mask_conf[gj, gi, best_n] = object_scale            fg_mask_coord[gj, gi, best_n] = 1            gt_weight_map[gj, gi, :] = weights[i]            gt_prior_map[gj, gi, best_n, 0] = gt_box[i, 0] - gi            gt_prior_map[gj, gi, best_n, 1] = gt_box[i, 1] - gj            gt_prior_map[gj, gi, best_n, 2] = gt_box[i, 2] / self.anchors[best_n, 0]            gt_prior_map[gj, gi, best_n, 3] = gt_box[i, 3] / self.anchors[best_n, 1]            gt_prior_map[gj, gi, best_n, 4] = 1  # gt anchor-based iou may not be accurate, so just use 0/1            # compute x,y coord canonical to current center            gt_prior_map[gj, gi, best_n, 5:] = anno[5:]            gt_prior_map[gj, gi, best_n, 5:5 + self.num_joints] -= gi            gt_prior_map[gj, gi, best_n, 5 + self.num_joints:5 + 2 * self.num_joints] -= gj            # normalize differently with different anchor sizes.            gt_prior_map[gj, gi, best_n, 5:5 + self.num_joints] /= (self.anchors[best_n, 0] / 2)            gt_prior_map[gj, gi, best_n, 5 + self.num_joints:5 + 2 * self.num_joints] /= (self.anchors[best_n, 1] / 2)        gt_prior_map = gt_prior_map.reshape((height, width, -1))        return gt_prior_map, fg_mask_conf, fg_mask_coord, gt_weight_map    def bbox_ious(self, boxes1, boxes2):        b1x1, b1y1 = np.hsplit(boxes1[:, :2] - (boxes1[:, 2:4] / 2), 2)        b1x2, b1y2 = np.hsplit(boxes1[:, :2] + (boxes1[:, 2:4] / 2), 2)        b2x1, b2y1 = np.hsplit(boxes2[:, :2] - (boxes2[:, 2:4] / 2), 2)        b2x2, b2y2 = np.hsplit(boxes2[:, :2] + (boxes2[:, 2:4] / 2), 2)        # broadcast        b1x1_cast = np.tile(b1x1.reshape(-1, 1), (1, len(b2x1)))        b1y1_cast = np.tile(b1y1.reshape(-1, 1), (1, len(b2y1)))        b1x2_cast = np.tile(b1x2.reshape(-1, 1), (1, len(b2x2)))        b1y2_cast = np.tile(b1y2.reshape(-1, 1), (1, len(b2y2)))        b2x1_cast = np.tile(b2x1.reshape(1, -1), (len(b1x1), 1))        b2y1_cast = np.tile(b2y1.reshape(1, -1), (len(b1y1), 1))        b2x2_cast = np.tile(b2x2.reshape(1, -1), (len(b1x2), 1))        b2y2_cast = np.tile(b2y2.reshape(1, -1), (len(b1y2), 1))        dx = np.minimum(b1x2_cast, b2x2_cast) - np.maximum(b1x1_cast, b2x1_cast)        dy = np.minimum(b1y2_cast, b2y2_cast) - np.maximum(b1y1_cast, b2y1_cast)        dx[dx < 0] = 0        dy[dy < 0] = 0        intersections = dx * dy        areas1 = (b1x2_cast - b1x1_cast) * (b1y2_cast - b1y1_cast)        areas2 = (b2x2_cast - b2x1_cast) * (b2y2_cast - b2y1_cast)        unions = (areas1 + areas2) - intersections        return intersections / unions    # This is the key function to prepare a set of target maps    def get_ground_truth(self, anns):        grid_h = int(self.input_y / self.stride)        grid_w = int(self.input_x / self.stride)        Agrid_h = int(self.input_y / self.strideA)        Agrid_w = int(self.input_x / self.strideA)        prior_h = int(self.input_y / self.stride_prior)        prior_w = int(self.input_x / self.stride_prior)        heatmaps = np.zeros((int(grid_h), int(grid_w), self.num_joints + 1))        align_maps = np.zeros((int(Agrid_h), int(Agrid_w), 2*len(self.joint_names)))        fg_masks_align = np.zeros((int(Agrid_h), int(Agrid_w), 2*len(self.joint_names)))        max_dist = 2 * (self.align_radius + 0.5)        dist_maps = np.ones((int(Agrid_h), int(Agrid_w), len(self.joint_names))).astype(np.float32) * max_dist        # This considers multiple annotations per frame later        keypoints_2d = []        pose_weights = []        objects = []        for ann in anns:            # need to get rid of those invisible joints            visible_joints = np.array(ann['visible_joints'])            if np.sum(visible_joints) == 0:                # print('here')                continue            keypoints_2d.append(np.array(ann['2d_joints']).reshape(-1, 2))            if 'pose_weight' in ann.keys():                pose_weights.append(ann['pose_weight'])            else:                pose_weights.append(1.0)            object = np.array([np.min(ann['2d_joints'][np.where(visible_joints), 0]) - joint2box_margin, np.min(ann['2d_joints'][np.where(visible_joints), 1]) - joint2box_margin,                               np.max(ann['2d_joints'][np.where(visible_joints), 0]) + joint2box_margin, np.max(ann['2d_joints'][np.where(visible_joints), 1]) + joint2box_margin, 1.0])            object = np.concatenate([object, ann['2d_joints'][:, 0].ravel()])            object = np.concatenate([object, ann['2d_joints'][:, 1].ravel()])            object = np.concatenate([object, ann['visible_joints'].ravel()])            objects.append(object)        keypoints_2d = np.array(keypoints_2d)        inbounds = self.remove_illegal_joint(keypoints_2d)  # indicate inbound of image, this removes those invisible joints assigned (-1, -1 ) position        objects = np.array(objects)        # prepare prior map and associated masks and weights        prior_map, prior_mask_conf, prior_mask_coord, prior_weight_map = \            self.build_prior_targets(objects, pose_weights, prior_h, prior_w)        """            Notes:                Truncation and occlusion should be considered separately                Occluded part needs to be prepared in regression maps so that they can be predicted in its channel.                This is ambiguity in multi-person overlapping of the same joint type.                So it require to label the occlusion caused by same joint type or others.        """        # confidence maps for body parts        for i in range(self.num_joints):            joints = keypoints_2d[:, i, :]            for j, joint in enumerate(joints):                # consider visible and within-range joint                if inbounds[j, i] > 0.5:                    root_center = joint[:2]                    gaussian_map = heatmaps[:, :, i]                    heatmaps[:, :, i] = putGaussianMaps(                        root_center, gaussian_map,                        7.0, grid_h, grid_w, self.stride)        # background: the last dimension of the heatmap prepared for background. Default for cross-entropy loss        heatmaps[:, :, -1] = np.maximum(            1 - np.max(heatmaps[:, :, :self.num_joints], axis=2),            0.        )        # TPDF maps        for j, joints in enumerate(keypoints_2d):            for k, joint in enumerate(joints):                if inbounds[j, k] < 0.5:                    continue                joint_center = joint[:2]                fg_mask_align = fg_masks_align[:, :, 2 * k: 2 * (k + 1)]                mapDxDy = align_maps[:, :, 2 * k: 2 * (k + 1)]                align_maps[:, :, 2 * k: 2 * (k + 1)],\                fg_masks_align[:, :, 2 * k: 2 * (k + 1)],\                dist_maps[:, :, k] = putJointDxDy(                    center=joint_center,                    depth=None,                    accumulate_map=mapDxDy,                    accumulate_mask=fg_mask_align,                    accumulate_dist=dist_maps[:, :, k],                    z_map=None,                    grid_y=Agrid_h, grid_x=Agrid_w, stride=self.strideA, radius=self.align_radius, max_dist=max_dist                )        return heatmaps, align_maps, fg_masks_align, prior_map, prior_mask_conf, prior_mask_coord, prior_weight_map    def __len__(self):        return len(self.ids)# unit test for dataloaderif __name__ == "__main__":    vis = True    input_size = 368    num_joints = 16    num_bones = 15    dim_reduction = 8    dim_reductionZ = 8    dim_reductionA = 8    batch_size = 5    AlignRadius = 3    anchor_h = (input_size/dim_reduction/2) - 3    anchors = [(anchor_h/2, anchor_h/4), (anchor_h, anchor_h/2)]    joint2chn = np.array(range(num_joints))    DATA_DIR = '/media/yuliang/DATA/Datasets/MPII'    img_dir = os.path.join(DATA_DIR, 'images')    ANNOTATIONS = '{}/labels_train.json'.format(os.path.join(DATA_DIR, 'labels'))    joint_names = get_keypoints()    preprocess = data_augmentation_2d3d.Compose([        data_augmentation_2d3d.Cvt2ndarray(num_joints=num_joints),        data_augmentation_2d3d.SquarePadRGB(),        data_augmentation_2d3d.RandomSacleRGB(),        data_augmentation_2d3d.Rotate(is_3d=False),        data_augmentation_2d3d.Hflip(swap_indices=get_swap_part_indices(), is_3d=False),        data_augmentation_2d3d.Crop(),        data_augmentation_2d3d.Resize(input_size)    ])    # # preprocess for testing    # preprocess = data_augmentation_depth_3d.Compose([    #     data_augmentation_depth_3d.Cvt2ndarray(),    #     data_augmentation_depth_3d.Resize(input_size)    # ])    val_data = MPII_Keypoints(        img_dir=img_dir,        ann_file=ANNOTATIONS,        preprocess=preprocess,        input_x=input_size,        input_y=input_size,        stride=dim_reduction,        stride_a=dim_reductionA,        anchors=anchors,        align_radius=AlignRadius    )    val_loader = torch.utils.data.DataLoader(        val_data, batch_size=batch_size, shuffle=False,        pin_memory=True, num_workers=0, drop_last=True)  # set num_wrkers=1 for debugging    cnt = 0    for i, (img, heatmap_target, alignmap_target, fg_masks_align, prior_map, prior_mask_conf, prior_mask_coord, prior_weight_map, index) in enumerate(val_loader):        alignmap_target = alignmap_target.cpu().data.numpy().transpose(0, 2, 3, 1)        prior_map = prior_map.cuda()        # evaluate the and visualize direct prior results        bboxes, humans_prior, visibilities = parse_prior_pose_rgb(prior_map,                                                                  anchors,                                                                  num_joints,                                                                  input_size,                                                                  input_size,                                                                  conf_threshold=0.1)        if vis is True:            for b in range(batch_size):                cnt += 1                single_img = img[b, :, :, :].numpy().transpose(1, 2, 0)                single_heatmap = heatmap_target[b, ...].numpy()                single_alignmap = alignmap_target[b, ...].transpose(2, 0, 1)                single_fg_mask_align = fg_masks_align[b, ...].numpy()                single_img *= rgb_std                single_img += rgb_mean                single_img[single_img < 0] = 0                single_img[single_img > 1] = 1                single_img *= 255                # single_img = single_img.astype(np.uint8)                # single_img = cv2.cvtColor(single_img.astype(np.uint8), cv2.COLOR_BGR2RGB)                single_img = cv2.cvtColor(single_img.astype(np.uint8), cv2.COLOR_RGB2BGR)                single_img_copy = np.copy(single_img)                fig1 = plt.figure()                # draw humans on the image                humans_2d = [humans_prior[b][hi][:, :2] for hi in range(len(humans_prior[b]))]                for j in range(len(bboxes[b])):                    single_bbox = bboxes[b][j]                    cv2.rectangle(single_img, (int(single_bbox[0]), int(single_bbox[1])),                                  (int(single_bbox[2]), int(single_bbox[3])), (0, 255, 0), 2)                single_img = draw_humans(single_img,                                         humans_2d,                                         kp_connections(get_keypoints()),                                         jointColors,                                         visibilities[b])                single_img = np.flip(single_img, 2)                ax6 = fig1.add_subplot(221)                ax6.imshow(single_img)                ax6.set_title('skeleton')                # visualize heatmap                ht_max = np.max(single_heatmap[:-1, :, :], axis=0)                ax2 = fig1.add_subplot(223)                ax2.imshow(ht_max)                ax2.set_title('ht max')                # visualize dx dy field maps, now only visualize for a single part                x_grids, y_grids = np.meshgrid(np.linspace(0,                                                           int(input_size/dim_reductionA)-1,                                                           int(input_size/dim_reductionA)),                                               np.linspace(0,                                                           int(input_size/dim_reductionA)-1,                                                           int(input_size/dim_reductionA)))                # x_grids += 0.5                # y_grids += 0.5                x_grids *= dim_reductionA                y_grids *= dim_reductionA                ##################################################################################################                joint_id = joint_names.index('WRIST_RIGHT')                dx_field = single_alignmap[2 * joint_id, :, :] * (AlignRadius+0.5)                dy_field = single_alignmap[2 * joint_id + 1, :, :] * (AlignRadius+0.5)                dist_field_vis1 = np.sqrt(dx_field ** 2 + dy_field ** 2)                # dx_field /= dist_field_vis1                # dy_field /= dist_field_vis1                dx_field *= dim_reductionA                dy_field *= dim_reductionA                color = np.sqrt(dy_field**2 + dx_field**2)                flow_color = flow_vis.flow_uv_to_colors(dx_field, dy_field, True)                ax7 = fig1.add_subplot(222)                # fig2, ax7 = plt.subplots(figsize=(9, 9))                ax7.imshow(np.flip(single_img_copy, 2))                for yy in range(dx_field.shape[0]):                    for xx in range(dx_field.shape[1]):                        ax7.quiver(x_grids[yy, xx], y_grids[yy, xx], dx_field[yy, xx], dy_field[yy, xx],                                   color=flow_color[yy, xx][::-1].astype(np.float)/255, angles='xy', units='xy',                                   scale=1.8, scale_units='xy')                ax7.set_aspect('equal')                ax7.xaxis.set_ticks([])                ax7.yaxis.set_ticks([])                # plt.savefig('{:06d}_PDF_right_wrist.pdf'.format(cnt), bboxx_inche='tight')                ax7.set_title('dist field right wrist')                sub_indices = np.logical_and(flow_color[:, :, 0] == 255,                                             np.logical_and(flow_color[:, :, 1] == 255, flow_color[:, :, 2] == 255))                flow_color[:, :, 0][sub_indices] = 0                flow_color[:, :, 1][sub_indices] = 0                flow_color[:, :, 2][sub_indices] = 0                flow_map = superimpose_colormap_on_img(single_img_copy, flow_color)                cv2.imshow('disp field right wrist', flow_map)                # cv2.imwrite('{:06d}_PDF_right_wrist.jpg'.format(cnt), flow_map)                ##################################################################################################                joint_id = joint_names.index('ANKLE_RIGHT')                dx_field = single_alignmap[2 * joint_id, :, :] * (AlignRadius+0.5)                dy_field = single_alignmap[2 * joint_id + 1, :, :] * (AlignRadius+0.5)                dist_field_vis2 = np.sqrt(dx_field ** 2 + dy_field ** 2)                # dx_field /= dist_field_vis2                # dy_field /= dist_field_vis2                dx_field *= dim_reductionA                dy_field *= dim_reductionA                color = np.sqrt(dy_field**2 + dx_field**2)                flow_color = flow_vis.flow_uv_to_colors(dx_field, dy_field, True)                ax8 = fig1.add_subplot(224)                # fig3, ax8 = plt.subplots(figsize=(9, 9))                ax8.imshow(np.flip(single_img_copy, 2))                for yy in range(dx_field.shape[0]):                    for xx in range(dx_field.shape[1]):                        ax8.quiver(x_grids[yy, xx], y_grids[yy, xx], dx_field[yy, xx], dy_field[yy, xx],                                   color=flow_color[yy, xx][::-1].astype(np.float)/255, angles='xy', units='xy',                                   scale=1.8, scale_units='xy')                ax8.set_aspect('equal')                ax8.xaxis.set_ticks([])                ax8.yaxis.set_ticks([])                # plt.savefig('{:06d}_PDF_right_ankle.pdf'.format(cnt), bboxx_inche='tight')                ax8.set_title('dist field right ankle')                sub_indices = np.logical_and(flow_color[:, :, 0] == 255,                                             np.logical_and(flow_color[:, :, 1] == 255, flow_color[:, :, 2] == 255))                flow_color[:, :, 0][sub_indices] = 0                flow_color[:, :, 1][sub_indices] = 0                flow_color[:, :, 2][sub_indices] = 0                flow_map = superimpose_colormap_on_img(single_img_copy, flow_color)                cv2.imshow('disp field right ankle', flow_map)                # cv2.imwrite('{:06d}_PDF_right_ankle.jpg'.format(cnt), flow_map)                fig1.subplots_adjust(wspace=0.1, hspace=0.01)                # print('pose weight: {}'.format(pose_weights[b]))                fig1.show()                plt.waitforbuttonpress()                plt.close(fig1)                # # visualize heat map over each joint separately                # for j in range(num_joints):                #     color_j = jointColors[j]                #     ht_j = np.copy(single_heatmap[j, :, :])                #     ht_j = np.repeat(ht_j[:, :, np.newaxis], 3, axis=2)                #     ht_j[:, :, 0] *= color_j[2]                #     ht_j[:, :, 1] *= color_j[1]                #     ht_j[:, :, 2] *= color_j[0]                #                #     ht_image_j = superimpose_colormap_on_img(single_img_copy, ht_j)                #                #     cv2.imshow('joint heatmap:', ht_image_j)                #     cv2.waitKey()                #     # cv2.imwrite('{:06d}_ht_{:02d}.jpg'.format(cnt, j), ht_image_j)        print('iter: [{}/{}]\t'.format(i, len(val_loader)))